{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiMUa1HcvjQS"
      },
      "source": [
        "#Dear chan (DRCH novel) and my **GPTQCustomLORA**\n",
        "Main Article (RU): https://habr.com/ru/articles/751972/\n",
        "\n",
        "Creator: https://github.com/CodeDruidX\n",
        "\n",
        "Src text: https://github.com/CodeDruidX/Everlasting-Summer-txt\n",
        "\n",
        "Fork from: https://github.com/qwopqwop200/gptqlora\n",
        "\n",
        "Origin: https://github.com/artidoro/qlora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/qwopqwop200/gptqlora\n",
        "!pip install auto-gptq\n",
        "!pip install accelerate\n",
        "!pip install evaluate\n",
        "!pip install transformers==4.31.0\n",
        "!pip install peft==0.4.0"
      ],
      "metadata": {
        "id": "lgEuNClGk3FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture mutedtoo\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"gurgutan/ruGPT-13B-4bit\",local_dir=r\"/ruGPT-3.5\")"
      ],
      "metadata": {
        "id": "O0segMVIor3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Your function to create datasets.Dataset\n",
        "def prepare_dataset():\n",
        "  from transformers import TextDataset\n",
        "  from datasets import Dataset\n",
        "  with open(\"/gptqlora/data.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    l=f.readlines()\n",
        "  #l=l.replace(\"\\n\",\"\\n\\n\")\n",
        "  import random\n",
        "\n",
        "  def get_random_substring(input_string, length):\n",
        "      start = random.randrange(0, len(input_string) - length + 1)\n",
        "      return input_string[start : start + length]\n",
        "  #print(get_random_substring(l,500))\n",
        "  l2=list(set([\"\\n\".join(get_random_substring(l,random.randint(4,6))) for i in range(2000)]))\n",
        "  print(len(l2),len(set(l2)))\n",
        "  l = list(map(lambda x: {\n",
        "              'input': '',\n",
        "              'output': x\n",
        "          },l2))\n",
        "  print(l2)\n",
        "  dataset=Dataset.from_list(l)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "pjRTSMUCTe2c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv=['gptqlora.py', '–learning_rate', '0.0001', '--model_path', '/ruGPT-3.5', '--max_steps', '300', '--dataset', 'CUSTOM-BABY', '--per_device_train_batch_size', '1', '--logging_steps', '5', '--save_steps', '20', '--output_dir', './output2']"
      ],
      "metadata": {
        "id": "JxBo5TKbZKrv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPTQCustomLora\n",
        "# This source code is licensed under the MIT license\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists, join, isdir\n",
        "from dataclasses import dataclass, field\n",
        "import sys\n",
        "from typing import Optional, Dict, Sequence\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    LineByLineTextDataset,\n",
        "    set_seed,\n",
        "    Seq2SeqTrainer,\n",
        "    LlamaTokenizerFast\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftModel\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from auto_gptq.utils.peft_utils import get_gptq_peft_model, GPTQLoraConfig\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from auto_gptq.nn_modules.qlinear import GeneralQuantLinear\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def prepare_model_for_int8_training(model, use_gradient_checkpointing=True):\n",
        "    r\"\"\"\n",
        "    This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
        "        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n",
        "        head to fp32\n",
        "\n",
        "    Args:\n",
        "        model, (`transformers.PreTrainedModel`):\n",
        "            The loaded model from `transformers`\n",
        "    \"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        # freeze base model's layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if use_gradient_checkpointing:\n",
        "        # For backward compatibility\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        else:\n",
        "\n",
        "            def make_inputs_require_grad(module, input, output):\n",
        "                output.requires_grad_(True)\n",
        "\n",
        "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "        # enable gradient checkpointing for memory efficiency\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    return model\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_path: Optional[str] = field(\n",
        "        default=\"./llama-7b/\"\n",
        "    )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enable unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    eval_dataset_size: int = field(\n",
        "        default=1024, metadata={\"help\": \"Size of validation dataset.\"}\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    source_max_len: int = field(\n",
        "        default=1024,\n",
        "        metadata={\"help\": \"Maximum source sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    target_max_len: int = field(\n",
        "        default=256,\n",
        "        metadata={\"help\": \"Maximum target sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    dataset: str = field(\n",
        "        default='alpaca',\n",
        "        metadata={\"help\": \"Which dataset to finetune on. See datamodule for options.\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None\n",
        "    )\n",
        "    train_on_source: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to train on the input in addition to the target text.\"}\n",
        "    )\n",
        "    mmlu_split: Optional[str] = field(\n",
        "        default='eval',\n",
        "        metadata={\"help\": \"The MMLU split to run on\"}\n",
        "    )\n",
        "    mmlu_dataset: Optional[str] = field(\n",
        "        default='mmlu-fs',\n",
        "        metadata={\"help\": \"MMLU dataset to use: options are `mmlu-zs` for zero-shot or `mmlu-fs` for few shot.\"}\n",
        "    )\n",
        "    do_mmlu_eval: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to run the MMLU evaluation.\"}\n",
        "    )\n",
        "    max_mmlu_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If set, only evaluates on `max_mmlu_samples` of the MMMLU dataset.\"}\n",
        "    )\n",
        "    mmlu_source_max_len: int = field(\n",
        "        default=2048,\n",
        "        metadata={\"help\": \"Maximum source sequence length for mmlu.\"}\n",
        "    )\n",
        "    full_finetune: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Finetune the entire model without adapters.\"}\n",
        "    )\n",
        "    adam8bit: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Use 8-bit adam.\"}\n",
        "    )\n",
        "    lora_r: int = field(\n",
        "        default=64,\n",
        "        metadata={\"help\": \"Lora R dimension.\"}\n",
        "    )\n",
        "    lora_alpha: float = field(\n",
        "        default=16,\n",
        "        metadata={\"help\": \" Lora alpha.\"}\n",
        "    )\n",
        "    lora_dropout: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\"help\":\"Lora dropout.\"}\n",
        "    )\n",
        "    max_memory_MB: int = field(\n",
        "        default=24000,\n",
        "        metadata={\"help\": \"Free memory per gpu.\"}\n",
        "    )\n",
        "    report_to: str = field(\n",
        "        default='none',\n",
        "        metadata={\"help\": \"To use wandb or something else for reporting.\"}\n",
        "    )\n",
        "    output_dir: str = field(default='./output', metadata={\"help\": 'The output dir for logs and checkpoints'})\n",
        "    optim: str = field(default='paged_adamw_32bit', metadata={\"help\": 'The optimizer to be used'})\n",
        "    per_device_train_batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n",
        "    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n",
        "    max_steps: int = field(default=10000, metadata={\"help\": 'How many optimizer update steps to take'})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) # use lora dropout instead for regularization if needed\n",
        "    learning_rate: float = field(default=0.0002, metadata={\"help\": 'The learnign rate'})\n",
        "    remove_unused_columns: bool = field(default=False, metadata={\"help\": 'Removed unused columns. Needed to make this codebase work.'})\n",
        "    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n",
        "    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n",
        "    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n",
        "    lr_scheduler_type: str = field(default='constant', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n",
        "    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n",
        "    logging_steps: int = field(default=10, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n",
        "    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n",
        "    save_strategy: str = field(default='steps', metadata={\"help\": 'When to save checkpoints'})\n",
        "    save_steps: int = field(default=250, metadata={\"help\": 'How often to save a model'})\n",
        "    save_total_limit: int = field(default=40, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n",
        "\n",
        "@dataclass\n",
        "class GenerationArguments:\n",
        "    # For more hyperparameters check:\n",
        "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
        "    # Length arguments\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default=256,\n",
        "        metadata={\"help\": \"Maximum number of new tokens to be generated in evaluation or prediction loops\"\n",
        "                          \"if predict_with_generate is set.\"}\n",
        "    )\n",
        "    min_new_tokens : Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Minimum number of new tokens to generate.\"}\n",
        "    )\n",
        "\n",
        "    # Generation strategy\n",
        "    do_sample: Optional[bool] = field(default=False)\n",
        "    num_beams: Optional[int] = field(default=1)\n",
        "    num_beam_groups: Optional[int] = field(default=1)\n",
        "    penalty_alpha: Optional[float] = field(default=None)\n",
        "    use_cache: Optional[bool] = field(default=True)\n",
        "\n",
        "    # Hyperparameters for logit manipulation\n",
        "    temperature: Optional[float] = field(default=1.0)\n",
        "    top_k: Optional[int] = field(default=50)\n",
        "    top_p: Optional[float] = field(default=1.0)\n",
        "    typical_p: Optional[float] = field(default=1.0)\n",
        "    diversity_penalty: Optional[float] = field(default=0.0)\n",
        "    repetition_penalty: Optional[float] = field(default=1.0)\n",
        "    length_penalty: Optional[float] = field(default=1.0)\n",
        "    no_repeat_ngram_size: Optional[int] = field(default=0)\n",
        "\n",
        "def find_all_linear_names(args, model):\n",
        "    cls = GeneralQuantLinear if not(args.full_finetune) else torch.nn.Linear\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "class SavePeftModelCallback(transformers.TrainerCallback):\n",
        "    def save_model(self, args, state, kwargs):\n",
        "        print('Saving PEFT checkpoint...')\n",
        "        if state.best_model_checkpoint is not None:\n",
        "            checkpoint_folder = os.path.join(state.best_model_checkpoint, \"adapter_model\")\n",
        "        else:\n",
        "            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        self.save_model(args, state, kwargs)\n",
        "        return control\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        def touch(fname, times=None):\n",
        "            with open(fname, 'a'):\n",
        "                os.utime(fname, times)\n",
        "\n",
        "        touch(join(args.output_dir, 'completed'))\n",
        "        self.save_model(args, state, kwargs)\n",
        "\n",
        "def get_accelerate_model(args, checkpoint_dir):\n",
        "\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{args.max_memory_MB}MB'\n",
        "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
        "\n",
        "    if args.full_finetune: assert args.bits in [16, 32]\n",
        "\n",
        "    print(f'loading base model {args.model_path}...')\n",
        "    model = AutoGPTQForCausalLM.from_quantized(\n",
        "        args.model_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map='auto',\n",
        "        max_memory=max_memory,\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "        inject_fused_attention = True,\n",
        "        inject_fused_mlp = False,\n",
        "        use_triton=True,\n",
        "        warmup_triton=False,\n",
        "        trainable=True\n",
        "    )\n",
        "    model.model.quantize_config = model.quantize_config\n",
        "    model.train()\n",
        "\n",
        "    setattr(model, 'model_parallel', True)\n",
        "    setattr(model, 'is_parallelizable', True)\n",
        "    #modules = find_all_linear_names(args, model)\n",
        "\n",
        "    model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
        "\n",
        "    if not args.full_finetune:\n",
        "        model = prepare_model_for_int8_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
        "    if args.gradient_checkpointing:\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    config = GPTQLoraConfig(\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        #target_modules=modules,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    if not args.full_finetune:\n",
        "        if checkpoint_dir is not None:\n",
        "            print(\"Loading adapters from checkpoint.\")\n",
        "            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'))\n",
        "            for name, p in model.named_parameters():\n",
        "                if 'lora' in name:\n",
        "                    print(name, p.sum())\n",
        "        else:\n",
        "            print(f'adding LoRA modules...')\n",
        "            model = get_gptq_peft_model(model, config, auto_find_all_linears=True, train_mode=True)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        else:\n",
        "            def make_inputs_require_grad(module, input, output):\n",
        "                output.requires_grad_(True)\n",
        "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LoraLayer):\n",
        "            if args.bf16:\n",
        "                module = module.to(torch.bfloat16)\n",
        "        if 'norm' in name:\n",
        "            module = module.to(torch.float32)\n",
        "        if 'lm_head' in name or 'embed_tokens' in name:\n",
        "            if hasattr(module, 'weight'):\n",
        "                if args.bf16 and module.weight.dtype == torch.float32:\n",
        "                    module = module.to(torch.bfloat16)\n",
        "    return model\n",
        "\n",
        "def print_trainable_parameters(args, model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    try:\n",
        "        trainable_params /= (32//model.quantize_config.bits)\n",
        "    except:\n",
        "        pass\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param}\")\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "    source_max_len: int\n",
        "    target_max_len: int\n",
        "    train_on_source: bool\n",
        "    predict_with_generate: bool\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract elements\n",
        "        sources = [example['input'] for example in instances]\n",
        "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
        "        # Tokenize\n",
        "        tokenized_sources_with_prompt = self.tokenizer(\n",
        "            sources,\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "        )\n",
        "        tokenized_targets = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        # Build the input and labels for causal LM\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "        for tokenized_source, tokenized_target in zip(\n",
        "            tokenized_sources_with_prompt['input_ids'],\n",
        "            tokenized_targets['input_ids']\n",
        "        ):\n",
        "            if not self.predict_with_generate:\n",
        "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
        "                if not self.train_on_source:\n",
        "                    labels.append(\n",
        "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
        "                    )\n",
        "                else:\n",
        "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
        "            else:\n",
        "                input_ids.append(torch.tensor(tokenized_source))\n",
        "        # Apply padding\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
        "        data_dict = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        }\n",
        "        if labels is not None:\n",
        "            data_dict['labels'] = labels\n",
        "        return data_dict\n",
        "\n",
        "def extract_unnatural_instructions_data(examples, extract_reformulations=False):\n",
        "    out = {\n",
        "        'input': [],\n",
        "        'output': [],\n",
        "    }\n",
        "    for example_instances in examples['instances']:\n",
        "        for instance in example_instances:\n",
        "            out['input'].append(instance['instruction_with_input'])\n",
        "            out['output'].append(instance['output'])\n",
        "    if extract_reformulations:\n",
        "        for example_reformulations in examples['reformulations']:\n",
        "            if example_reformulations is not None:\n",
        "                for instance in example_reformulations:\n",
        "                    out['input'].append(instance['instruction_with_input'])\n",
        "                    out['output'].append(instance['output'])\n",
        "    return out\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def extract_alpaca_dataset(example):\n",
        "    if example.get(\"input\", \"\") != \"\":\n",
        "        prompt_format = PROMPT_DICT[\"prompt_input\"]\n",
        "    else:\n",
        "        prompt_format = PROMPT_DICT[\"prompt_no_input\"]\n",
        "    return {'input': prompt_format.format(**example)}\n",
        "\n",
        "\n",
        "def get_last_checkpoint(checkpoint_dir):\n",
        "    if isdir(checkpoint_dir):\n",
        "        is_completed = exists(join(checkpoint_dir, 'completed'))\n",
        "        if is_completed: return None, True # already finished\n",
        "        max_step = 0\n",
        "        for filename in os.listdir(checkpoint_dir):\n",
        "            if isdir(join(checkpoint_dir, filename)) and filename.startswith('checkpoint'):\n",
        "                max_step = max(max_step, int(filename.replace('checkpoint-', '')))\n",
        "        if max_step == 0: return None, is_completed # training started, but no checkpoint\n",
        "        checkpoint_dir = join(checkpoint_dir, f'checkpoint-{max_step}')\n",
        "        print(f\"Found a previous checkpoint at: {checkpoint_dir}\")\n",
        "        return checkpoint_dir, is_completed # checkpoint found!\n",
        "    return None, False # first training\n",
        "\n",
        "def train():\n",
        "    hfparser = transformers.HfArgumentParser((\n",
        "        ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
        "    ))\n",
        "    model_args, data_args, training_args, generation_args, extra_args = \\\n",
        "        hfparser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
        "    training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
        "    args = argparse.Namespace(\n",
        "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
        "    )\n",
        "    print(args)\n",
        "\n",
        "    checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
        "    if completed_training:\n",
        "        print('Detected that training was already completed!')\n",
        "\n",
        "    model = get_accelerate_model(args, checkpoint_dir)\n",
        "    training_args.skip_loading_checkpoint_weights=True\n",
        "\n",
        "    resume_from_checkpoint = checkpoint_dir\n",
        "    if resume_from_checkpoint:\n",
        "        # Check the available weights and load them\n",
        "        checkpoint_name = os.path.join(\n",
        "            checkpoint_dir, \"pytorch_model.bin\"\n",
        "        )  # Full checkpoint\n",
        "        if not os.path.exists(checkpoint_name):\n",
        "            checkpoint_path = os.path.join(\n",
        "                checkpoint_dir, \"adapter_model\"\n",
        "            )\n",
        "\n",
        "            checkpoint_name = os.path.join(\n",
        "                checkpoint_path, \"adapter_model.bin\"\n",
        "            )  # only LoRA model - LoRA config above has to fit\n",
        "            resume_from_checkpoint = (\n",
        "                False  # So the trainer won't try loading its state\n",
        "            )\n",
        "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
        "        if os.path.exists(checkpoint_name):\n",
        "            print(f\"Restarting from {checkpoint_name}\")\n",
        "            adapters_weights = torch.load(checkpoint_name)\n",
        "            set_peft_model_state_dict(model, adapters_weights)\n",
        "        else:\n",
        "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    print_trainable_parameters(args, model)\n",
        "    print('loaded model')\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.model_path,\n",
        "        cache_dir=args.cache_dir,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=True,\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "    if isinstance(tokenizer, LlamaTokenizerFast):\n",
        "        # LLaMA tokenizer may not have correct special tokens set.\n",
        "        # Check and add them if missing to prevent them from being parsed into different tokens.\n",
        "        # Note that these are present in the vocabulary.\n",
        "        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
        "                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
        "                \"unk_token\": tokenizer.convert_ids_to_tokens(model.config.pad_token_id),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    if not args.full_finetune:\n",
        "        trainer.add_callback(SavePeftModelCallback)\n",
        "    if args.do_mmlu_eval:\n",
        "        if args.mmlu_dataset == 'mmlu-zs':\n",
        "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
        "                'eval': 'data/mmlu/zero_shot_mmlu_val.json',\n",
        "                'test': 'data/mmlu/zero_shot_mmlu_test.json',\n",
        "            })\n",
        "            mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
        "        # MMLU Five-shot (Eval/Test only)\n",
        "        elif args.mmlu_dataset == 'mmlu' or args.mmlu_dataset == 'mmlu-fs':\n",
        "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
        "                'eval': 'data/mmlu/five_shot_mmlu_val.json',\n",
        "                'test': 'data/mmlu/five_shot_mmlu_test.json',\n",
        "            })\n",
        "            # mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
        "        mmlu_dataset = mmlu_dataset[args.mmlu_split]\n",
        "        if args.max_mmlu_samples is not None:\n",
        "            mmlu_dataset = mmlu_dataset.select(range(args.max_mmlu_samples))\n",
        "        abcd_idx = [\n",
        "            tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n",
        "        ]\n",
        "        accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "        class MMLUEvalCallback(transformers.TrainerCallback):\n",
        "            def on_evaluate(self, args, state, control, model, **kwargs):\n",
        "                data_loader = trainer.get_eval_dataloader(mmlu_dataset)\n",
        "                source_max_len = trainer.data_collator.source_max_len\n",
        "                trainer.data_collator.source_max_len = args.mmlu_source_max_len\n",
        "                trainer.model.eval()\n",
        "                preds, refs = [], []\n",
        "                loss_mmlu = 0\n",
        "                for batch in tqdm(data_loader, total=len(data_loader)):\n",
        "                    (loss, logits, labels) = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n",
        "                    # There are two tokens, the output, and eos token.\n",
        "                    for i, logit in enumerate(logits):\n",
        "                        label_non_zero_id = (batch['labels'][i] != -100).nonzero()[0][0]\n",
        "                        logit_abcd = logit[label_non_zero_id-1][abcd_idx]\n",
        "                        preds.append(torch.argmax(logit_abcd).item())\n",
        "                    labels = labels[labels != IGNORE_INDEX].view(-1, 2)[:,0]\n",
        "                    for label in labels.tolist():\n",
        "                        if label in abcd_idx:\n",
        "                            refs += [abcd_idx.index(label)]\n",
        "\n",
        "                    loss_mmlu += loss.item()\n",
        "                # Extract results by subject.\n",
        "                results = {'mmlu_loss':loss_mmlu/len(data_loader)}\n",
        "                subject = mmlu_dataset['subject']\n",
        "                subjects = {s:{'refs':[], 'preds':[]} for s in set(subject)}\n",
        "                for s,p,r in zip(subject, preds, refs):\n",
        "                    subjects[s]['preds'].append(p)\n",
        "                    subjects[s]['refs'].append(r)\n",
        "                subject_scores = []\n",
        "                for subject in subjects:\n",
        "                    subject_score = accuracy.compute(\n",
        "                        references=subjects[subject]['refs'],\n",
        "                        predictions=subjects[subject]['preds']\n",
        "                    )['accuracy']\n",
        "                    results[f'mmlu_{args.mmlu_split}_accuracy_{subject}'] = subject_score\n",
        "                    subject_scores.append(subject_score)\n",
        "                results[f'mmlu_{args.mmlu_split}_accuracy'] = np.mean(subject_scores)\n",
        "                trainer.log(results)\n",
        "                trainer.data_collator.source_max_len = source_max_len\n",
        "\n",
        "        trainer.add_callback(MMLUEvalCallback)\n",
        "\n",
        "    # Verifying the datatypes.\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    all_metrics = {\"run_name\": args.run_name}\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        all_metrics.update(metrics)\n",
        "    # Evaluation\n",
        "    if args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "        all_metrics.update(metrics)\n",
        "    # Prediction\n",
        "    if args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "        prediction_output = trainer.predict(test_dataset=data_module['predict_dataset'],metric_key_prefix=\"predict\")\n",
        "        prediction_metrics = prediction_output.metrics\n",
        "        predictions = prediction_output.predictions\n",
        "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "        predictions = tokenizer.batch_decode(\n",
        "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        with open(os.path.join(args.output_dir, 'predictions.jsonl'), 'w') as fout:\n",
        "            for i, example in enumerate(data_module['predict_dataset']):\n",
        "                example['prediction_with_input'] = predictions[i].strip()\n",
        "                example['prediction'] = predictions[i].replace(example['input'], '').strip()\n",
        "                fout.write(json.dumps(example) + '\\n')\n",
        "        print(prediction_metrics)\n",
        "        trainer.log_metrics(\"predict\", prediction_metrics)\n",
        "        trainer.save_metrics(\"predict\", prediction_metrics)\n",
        "        all_metrics.update(prediction_metrics)\n",
        "\n",
        "    if (args.do_train or args.do_eval or args.do_predict):\n",
        "        with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
        "            fout.write(json.dumps(all_metrics))\n",
        "\n",
        "\n",
        "def make_data_module(tokenizer: transformers.PreTrainedTokenizer, args) -> Dict:\n",
        "    # From origin author:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Make dataset and collator for supervised fine-tuning.\n",
        "    Datasets are expected to have the following columns: { `input`, `output` }\n",
        "\n",
        "    Available datasets to be selected with `dataset` argument:\n",
        "        - alpaca, 52002 examples\n",
        "        - alpaca cleaned, 51942 examples\n",
        "        - chip2 (OIG), 210289 examples\n",
        "        - self-instruct, 82612 examples\n",
        "        - hh-rlhf (Anthropic), 160800 examples\n",
        "        - longform, 23.7k examples\n",
        "\n",
        "    Coming soon:\n",
        "        - unnatural instructions core, 66010 examples\n",
        "        - unnatural instructions full, 240670 examples\n",
        "        - alpaca-gpt4, 52002 examples\n",
        "        - unnatural-instructions-gpt4, 9000 examples\n",
        "        - oa-rlhf (OpenAssistant) primary message tree only, 9209 examples\n",
        "        - oa-rlhf-assistant (OpenAssistant) all assistant  replies with ranking\n",
        "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
        "        - flan (FLAN v2), up to 20M examples available\n",
        "\n",
        "    Not Available:\n",
        "        - vicuna, not released at the moment.\n",
        "    \"\"\"\n",
        "    # From me:\n",
        "    \"\"\"\n",
        "    Just keep 'CUSTOM-BABY' as arg and edit prepare_dataset()\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Load dataset.\n",
        "    # Alpaca\n",
        "    print(args.dataset)\n",
        "\n",
        "    if args.dataset == 'CUSTOM-BABY':\n",
        "      dataset=prepare_dataset()\n",
        "      print(dataset)\n",
        "      #train_dataset=LineByLineTextDataset(tokenizer=tokenizer,file_path=\"/gptqlora/datafromES.txt\",block_size=64)\n",
        "      #print(train_dataset.examples)\n",
        "      #train_dataset.examples = list(map(lambda x: {\n",
        "      #      'input': '',\n",
        "      #      'output': x\n",
        "      #  },train_dataset.examples))\n",
        "    #elif args.dataset == 'alpaca':\n",
        "    # Alpaca clean\n",
        "    elif args.dataset == 'alpaca-clean':\n",
        "        dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
        "        dataset = dataset.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
        "    # Chip2\n",
        "    elif args.dataset == 'chip2':\n",
        "        dataset = load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
        "        dataset = dataset.map(lambda x: {\n",
        "            'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
        "            'output': x['text'].split('\\n<bot>: ')[1],\n",
        "        }, remove_columns=['text', 'metadata'])\n",
        "    # Self Instruct\n",
        "    elif args.dataset == 'self-instruct':\n",
        "        dataset = load_dataset(\"yizhongw/self_instruct\", name='self_instruct')\n",
        "        for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
        "            dataset = dataset.rename_column(old, new)\n",
        "    # Anthropic rlhf\n",
        "    elif args.dataset == 'hh-rlhf':\n",
        "        dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
        "        dataset = dataset.map(lambda x: {\n",
        "            'input': '',\n",
        "            'output': x['chosen']\n",
        "        }, remove_columns=['chosen', 'rejected'])\n",
        "        print(dataset)\n",
        "    # LongForm\n",
        "    elif args.dataset == 'longform':\n",
        "        dataset = load_dataset(\"akoksal/LongForm\")\n",
        "    elif args.dataset == 'vicuna':\n",
        "        raise NotImplementedError(\"Vicuna data was not released.\")\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Dataset {args.dataset} not implemented yet.\")\n",
        "\n",
        "    # Split train/eval, reduce size\n",
        "    if args.do_eval or args.do_predict:\n",
        "        if 'eval' in dataset:\n",
        "            eval_dataset = dataset['eval']\n",
        "        else:\n",
        "            print('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
        "            dataset = dataset[\"train\"].train_test_split(\n",
        "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
        "            )\n",
        "            eval_dataset = dataset['test']\n",
        "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
        "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
        "        if args.group_by_length:\n",
        "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "    if args.do_train:\n",
        "        #train_dataset = dataset['train']\n",
        "        train_dataset = dataset\n",
        "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
        "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "        if args.group_by_length:\n",
        "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "\n",
        "    data_collator = DataCollatorForCausalLM(\n",
        "        tokenizer=tokenizer,\n",
        "        source_max_len=args.source_max_len,\n",
        "        target_max_len=args.target_max_len,\n",
        "        train_on_source=args.train_on_source,\n",
        "        predict_with_generate=args.predict_with_generate,\n",
        "    )\n",
        "    return dict(\n",
        "        train_dataset=train_dataset if args.do_train else None,\n",
        "        eval_dataset=eval_dataset if args.do_eval else None,\n",
        "        predict_dataset=eval_dataset if args.do_predict else None,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WP8ETHl5Ro7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# ALL NEXT CODE IS NOT MATTER\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "7313uDsJUyyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJNzaJbeWWhB",
        "outputId": "8291f164-5daa-4ff3-cc28-57a8d4d6ae5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_N8D_Fvwf5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test model with lora\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import auto_gptq\n",
        "from auto_gptq import AutoGPTQForCausalLM, get_gptq_peft_model\n",
        "from auto_gptq.utils.peft_utils import GPTQLoraConfig\n",
        "#import gradio as gr\n",
        "\n",
        "model_name = 'fffrrt/ruGPT-3.5-13B-GPTQ'\n",
        "model_basename = 'gptq_model-4bit-128g'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoGPTQForCausalLM.from_quantized(\"gurgutan/ruGPT-13B-4bit\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "        inject_fused_attention = True,\n",
        "        inject_fused_mlp = False,\n",
        "        use_triton=True,\n",
        "        warmup_triton=False,\n",
        "        trainable=True)\n",
        "\n",
        "#t=torch.load('saved_model.pth')\n",
        "#model.load_state_dict(t['model_state_dict'])\n",
        "# loaded_tokenizer = t['tokenizer']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gptqlora.py -–learning_rate 0.0001 --model_path /ruGPT-3.5 --max_steps 300 --dataset CUSTOM-BABY --per_device_train_batch_size 1 --logging_steps 5 --save_steps 20 --output_dir ./output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1oNcZJk4US-",
        "outputId": "22593d0c-a5e0-46fa-b191-a8b3d51038a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-28 14:48:52.414767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading base model /ruGPT-3.5...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 164, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '/ruGPT-3.5'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 792, in <module>\n",
            "    train()\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 601, in train\n",
            "    model = get_accelerate_model(args, checkpoint_dir)\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 282, in get_accelerate_model\n",
            "    model = AutoGPTQForCausalLM.from_quantized(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py\", line 87, in from_quantized\n",
            "    model_type = check_and_get_model_type(model_name_or_path, trust_remote_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py\", line 147, in check_and_get_model_type\n",
            "    config = AutoConfig.from_pretrained(model_dir, trust_remote_code=trust_remote_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 983, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 617, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 693, in _get_config_dict\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load the configuration of '/ruGPT-3.5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/ruGPT-3.5' is the correct path to a directory containing a config.json file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = GPTQLoraConfig(\n",
        "    inference_mode=True,\n",
        ")\n",
        "model = get_gptq_peft_model(model, peft_config, '/content/drive/MyDrive/ckp/adapter_model/adapter_model')"
      ],
      "metadata": {
        "id": "wjv6Dlrjrp3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.is_loaded_in_4bit=True"
      ],
      "metadata": {
        "id": "0Kj_IL4KPYNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=torch.load('saved_model.pth',torch.device('cpu'))\n",
        "model.load_state_dict(t['model_state_dict'])\n",
        "tokenizer = t['tokenizer']\n",
        "\n",
        "del t\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrhA36FR7tA7",
        "outputId": "de97cf2b-0741-4871-99ab-a166ab8c6301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg={\n",
        "\"K\":40,\n",
        "\"P\":0.98,\n",
        "\"temperature\":0.6,\n",
        "\"uningram\":4\n",
        "}\n",
        "def editcfg(name,val):\n",
        "  print(name,val)\n",
        "  global cfg\n",
        "  cfg[name]=val\n",
        "\n",
        "def gen(text,tokens=10):\n",
        "  encoded_input = tokenizer(text, return_tensors='pt').to('cuda:0')\n",
        "  print(encoded_input.input_ids[:,-1900:].shape)\n",
        "  output = model.generate(\n",
        "      input_ids=encoded_input.input_ids[:,-1900:],\n",
        "      max_new_tokens=tokens,\n",
        "      do_sample=True,\n",
        "      top_k=cfg['K'],\n",
        "      top_p=cfg['P'],\n",
        "      temperature=cfg['temperature'],\n",
        "      #num_beams=4,\n",
        "      no_repeat_ngram_size=cfg['uningram'],\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      # num_return_sequences=5,\n",
        "      # do_sample=True\n",
        "      #repetition_penalty = 1.04\n",
        "  )\n",
        "\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "GHiktwGF9pKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oe8rQWd20QC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f930959-01ef-43b7-8b05-81643edf6cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n",
            "torch.Size([1, 277])\n",
            "torch.Size([1, 307])\n",
            "torch.Size([1, 337])\n",
            "torch.Size([1, 367])\n",
            "torch.Size([1, 397])\n",
            "torch.Size([1, 279])\n",
            "torch.Size([1, 309])\n",
            "torch.Size([1, 339])\n",
            "torch.Size([1, 369])\n",
            "torch.Size([1, 399])\n",
            "K 40\n",
            "P 0.98\n",
            "uningram 4\n",
            "temperature 0.2\n",
            "torch.Size([1, 22])\n",
            "torch.Size([1, 52])\n",
            "torch.Size([1, 28])\n",
            "torch.Size([1, 53])\n",
            "torch.Size([1, 83])\n",
            "torch.Size([1, 113])\n",
            "torch.Size([1, 143])\n"
          ]
        }
      ],
      "source": [
        "#@title UI\n",
        "import gradio as gr\n",
        "def complete_with_gpt(text):\n",
        "    text=gen(text,30)\n",
        "    return text\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Ever novel\")\n",
        "    textbox = gr.Textbox(placeholder=\"Верни контекст на место!\", value=\"123\")\n",
        "    btn = gr.Button(\"Ожидать\")\n",
        "    gr.Markdown(\"---\")\n",
        "    with gr.Row():\n",
        "      K=gr.Number(label=\"top_K\",value=cfg[\"K\"],minimum=1)\n",
        "      P=gr.Number(label=\"top_P\",value=cfg[\"P\"],minimum=0,maximum=1)\n",
        "      T=gr.Number(label=\"Temperature\",value=cfg[\"temperature\"],minimum=0)\n",
        "      U=gr.Number(label=\"Unique ngram len\",value=cfg[\"uningram\"],minimum=0)\n",
        "      APPLY=gr.Button(value=\"Применить\")\n",
        "\n",
        "    APPLY.click(fn=lambda x: editcfg(\"K\",int(x)),inputs=K)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"P\",x),inputs=P)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"temperature\",x),inputs=T)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"uningram\",int(x)),inputs=U)\n",
        "    btn.click(fn=complete_with_gpt, inputs=textbox, outputs=textbox, queue=False)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install peft\n",
        "#!pip install bitsandbytes\n",
        "#del peft\n",
        "from importlib import reload\n",
        "import peft\n",
        "peft=reload(peft)\n",
        "import bitsandbytes as bnb\n",
        "#from peft import LoraConfig, get_peft_model,  prepare_model_for_kbit_training\n",
        "import torch\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "config = peft.LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = peft.get_peft_model(model, config)\n",
        "\n",
        "model = peft.prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Сохраним обучающие данные в .txt файл\n",
        "train_path = 'train_dataset.txt'\n",
        "#with open(train_path, \"w\") as f:\n",
        "#    f.write(prompt)\n",
        "\n",
        "# Создание датасета\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\n",
        "\n",
        "# Создание даталодера (нарезает текст на оптимальные по длине куски)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
        "                                                mlm=False)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned2\", # The output directory\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output dir\n",
        "    num_train_epochs=0.01, # number of training epochs\n",
        "    per_device_train_batch_size=1, # batch size for training\n",
        "    auto_find_batch_size=True,\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    warmup_steps=10, # number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=1, # to make \"virtual\" batch size larger\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
        ")\n",
        "trainer.train()\n",
        "torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'tokenizer': tokenizer\n",
        "  }, 'saved_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "RdwvgGrxRmBy",
        "outputId": "61e9d90c-7b70-49a6-aab8-3e41edd440e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dd635c94f2db>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/mapping.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptLearningConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_prompt_learning_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptLearningConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# transformers models have a .config attribute, whose presence is assumed later on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36madd_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_and_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             raise ValueError(\n\u001b[1;32m    196\u001b[0m                 \u001b[0;34m\"LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m_find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_target_modules_in_base_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m_create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mloaded_in_4bit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bnb_4bit_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear4bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mfourbit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             fourbit_kwargs.update(\n\u001b[1;32m    265\u001b[0m                 {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bnb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = trainer.quantize()"
      ],
      "metadata": {
        "id": "DgOXolJpGHAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen('''Семён: Посмотри, он совсем не страшный.\n",
        "Лена выглянула у меня из-за спины.\n",
        "Лена: Не страшный…\n",
        "Семён: Сейчас, подожди.\n",
        "Я мягко освободился от её объятий и подошёл к совёнку.\n",
        "Сначала казалось, что он испугается и улетит, выпустив воланчик.\n",
        "Однако совёнок, похоже, и не собирался двигаться с места.\n",
        "Мне удалось схватить воланчик и аккуратно отобрать его у птицы.\n",
        "Семён: Смотри, он совсем ручной! Хочешь его погладить?\n",
        "Ленка взяла меня за руку.\n",
        "Мы осторожно приблизились к совёнку, который'''.replace(\"\\n\",\"\\n\\n\"),20)"
      ],
      "metadata": {
        "id": "yFDP10C5pf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "96d48659-ba65-4014-a727-5229025baf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 142])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Семён: Посмотри, он совсем не страшный.\\n\\nЛена выглянула у меня из-за спины.\\n\\nЛена: Не страшный…\\n\\nСемён: Сейчас, подожди.\\n\\nЯ мягко освободился от её объятий и подошёл к совёнку.\\n\\nСначала казалось, что он испугается и улетит, выпустив воланчик.\\n\\nОднако совёнок, похоже, и не собирался двигаться с места.\\n\\nМне удалось схватить воланчик и аккуратно отобрать его у птицы.\\n\\nСемён: Смотри, он совсем ручной! Хочешь его погладить?\\n\\nЛенка взяла меня за руку.\\n\\nМы осторожно приблизились к совёнку, который в это время начал клевать что-то у себя под ногами.\\n\\nДойдя до него'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_load_from_checkpoint(\"12123123\")"
      ],
      "metadata": {
        "id": "n9rSRXbhbJy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(\"\"\"from collections import defau\"\"\",20)"
      ],
      "metadata": {
        "id": "QwfeH0NwFIFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6d399410-97b7-4f72-8396-46f105f6fbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from collections import defauxt\\n This source code is licensed under the MIT license found in the\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Wvuziftlukg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./trained_model3/',use_safetensors=True)\n",
        "tokenizer.save_pretrained('./trained_model3/')"
      ],
      "metadata": {
        "id": "7lrBoRGDkryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo swapon /newswap"
      ],
      "metadata": {
        "id": "vGKWK999UOoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6vQZjR1irClJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()print(gen(prompt,30))\n"
      ],
      "metadata": {
        "id": "buVDt27B2Pyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
        "print(encoded_input.input_ids.shape)\n",
        "#print(len(encoded_input[0]))"
      ],
      "metadata": {
        "id": "ISZHSx5BGup7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UfrFZeN6mi4"
      },
      "outputs": [],
      "source": [
        "prompt=\"\"\"Мне снился сон…\n",
        "Казалось, я нахожусь в каком-то вакууме, а вокруг – пустота.\n",
        "Но не только _вокруг_ – Я единственное существо во Вселенной.\n",
        "Как будто она вернулась в некое сингулярное состояние перед самым Большим Взрывом.\n",
        "И вот-вот что-то должно было произойти.\n",
        "Вдруг я начал слышать голос.\n",
        "Слов не разобрать, но он показался мне знакомым.\n",
        "Голос что-то нежно нашёптывал, как будто убаюкивая.\n",
        "И тут я понял... Это был голос той Незнакомки из автобуса. Той девушки из сна.\n",
        "Мысли: Но что она хочет мне сказать? Кто она?..\n",
        "Я проснулся.\n",
        "Яркие лучи солнца били в глаза.\n",
        "Время приближалось к полудню.\n",
        "Лениво потянувшись и зевнув, я начал вспоминать вчерашний день.\n",
        "За несколько секунд все его события пронеслись перед глазами: автобус, лагерь, местные обитатели.\n",
        "Мысли: Но ведь всё не так, неправильно!\n",
        "Не эта ситуация, не моё положение – они неправильны априори, – а моё отношение к ним.\n",
        "Мысли: Ведь я вчера запросто заснул здесь, а до этого мило беседовал с местными пионерами, даже умудрялся шутить!\n",
        "Мысли: Но как можно себя так вести в подобной ситуации?!\n",
        "Мысли: Я же должен бояться, шарахаться от каждого шороха, избегать любого контакта с потенциально враждебными существами.\n",
        "Все события прошедшего дня словно заволокло похмельной дымкой.\n",
        "Мысли: Это очень похоже на утро после хорошей пьянки: вчерашнее естественное, непредосудительное, в высшей степени нормальное поведение утром становится кошмаром, гротескной гравюрой из иллюстраций к «Божественной комедии».\n",
        "Мысли: Да, всё именно так, однако прошлого уже не вернуть.\n",
        "Хотя, возможно, оценив обстановку, я действовал по ситуации.\n",
        "Я огляделся по сторонам, пытаясь понять, не забросило ли меня куда-нибудь ещё, но домик Ольги Дмитриевны выглядел так же, как и вчера.\n",
        "Всё было как будто на своих местах, разве что на спинке кровати висела пионерская форма.\n",
        "Я с недоверием покрутил её в руках, примерил и оделся.\n",
        "Мысли: Всё равно это лучше, чем ходить в зимней одежде.\n",
        "Мысли: Посмотреть бы теперь на себя – наверняка выгляжу как клоун!\n",
        "А для этого нужно зеркало. Хотя бы самое маленькое.\n",
        "Нашлось оно на дверце шкафа.\n",
        "Семён: Твою!..\n",
        "Я взглянул на новоиспечённого пионера и аж отпрыгнул в сторону от неожиданности!\n",
        "На другой стороне зеркала стоял какой-то подросток!\n",
        "Похожий на меня, но не я!\n",
        "Куда пропали недельная небритость, мешки под глазами, сутулость и смертельно уставшее выражение лица?!\n",
        "Похоже, меня не закинули назад во времени или в параллельную реальность, а просто поменяли с кем-то телами.\n",
        "Мысли: Действительно просто! Такое же на каждом шагу встречается!\n",
        "Я пригляделся к незнакомцу повнимательнее и только тогда понял, что это я сам!\n",
        "Только образца конца школы – начала института.\n",
        "Мысли: Ладно, хотя бы так.\n",
        "Мысли: Да уж, _человек в стрессовой ситуации_ слона и не приметил!\n",
        "Мысли: А вот вожатая обратила внимание и вчера ночью меня отчитала за неподобающее обращение к ней…\n",
        "Мысли: К чёрту!\n",
        "Мысли: Вряд ли мой внешний вид влияет на что-то ещё.\n",
        "Если верить часам, завтрак уже давно позади.\n",
        "Мысли: Ну ладно, попробую всё же в столовой что-нибудь найти.\n",
        "Мысли: Вчера же со Славей получилось.\n",
        "От этих воспоминаний я невольно улыбнулся.\n",
        "На улице ярко светило солнце, дул лёгкий ветерок.\n",
        "Мысли: Прекрасный летний день.\n",
        "Я уже несколько лет не чувствовал себя по утрам так хорошо.\n",
        "Все проблемы на секунду улетели куда-то далеко, растворились в редких, цвета первого снега облаках.\n",
        "Вдруг передо мной словно из ниоткуда появилась Ольга Дмитриевна.\n",
        "Ольга Дмитриевна: Доброе утро, Семён!\n",
        "Семён: Доброе!\n",
        "Я улыбнулся, пытаясь всем своим видом показать, что несмотря ни на что утро моё было действительно добрым.\n",
        "Ольга Дмитриевна: Ты только вчера приехал, так что будить я тебя не стала, но завтрак-то…\n",
        "Ольга Дмитриевна: Хотя ладно! Вот, держи!\n",
        "Она протянула мне бумажный свёрток.\n",
        "Судя по масляным пятнам, внутри, скорее всего, бутерброды.\n",
        "Семён: Ой, спасибо!\n",
        "Ольга Дмитриевна: А теперь марш умываться!\n",
        "Я уже собирался уходить.\n",
        "Ольга Дмитриевна: Сейчас, подожди.\n",
        "Ольга Дмитриевна забежала в домик и, вернувшись, сунула мне небольшой пакетик.\n",
        "Внутри оказались зубная щётка, мыло, небольшое полотенце и что-то ещё – я особо не всматривался.\n",
        "Ольга Дмитриевна: Пионер должен быть всегда чист и опрятен!\n",
        "Ольга Дмитриевна: Дай я тебе галстук правильно завяжу на первый раз, а то он болтается. Потом научишься, сам будешь!\n",
        "Семён: А может, не надо? Я сейчас умываться иду.\n",
        "Мысли: Ну да, вдруг зацеплюсь за кран и удавлюсь...\n",
        "Ольга Дмитриевна: Ладно, тогда потом. И не забудь про линейку.\n",
        "Мысли: Карандаши, ручки, линейки… Такие вещи не забываются!\n",
        "Семён: Какую линейку?\n",
        "Ольга Дмитриевна: В смысле – какую линейку?!\n",
        "Она нахмурилась.\n",
        "Ольга Дмитриевна: Сегодня же понедельник!\n",
        "Мысли: Странно, а по моим подсчётам – воскресенье…\n",
        "Мысли: Впрочем, смена дня недели – это ещё не самое страшное.\n",
        "Ольга Дмитриевна: Обычно у нас линейки рано утром, до завтрака, но сегодня понедельник, поэтому она будет в 12 часов.\n",
        "Ольга Дмитриевна: Не опаздывай!\n",
        "Семён: Хорошо. А где?\n",
        "Ольга Дмитриевна: На площади, где же ещё!\n",
        "Спорить было бессмысленно.\n",
        "Я направился в сторону «помывочной».\n",
        "На отдельный душ и туалет рассчитывать не приходилось, но при виде этого выкидыша загнивающего социализма – причудливой черепашки с панцирем из жести, множеством ног-кранов и кафельным брюшком – мне стало несколько не по себе.\n",
        "Я не был брезгливым человеком, но тем не менее, стоя тут, понял, что всё же есть какой-то минимальный уровень привычного комфорта, без которого жить мне довольно проблематично.\n",
        "Вот ведь как бывает – когда теряешь вещи, которые всегда казались совершенно обыденными и естественными, понимаешь, что на самом деле они были незаменимы.\n",
        "Мысли: А, да и чёрт с ним! Выбирать всё равно не из чего.\n",
        "Вода оказалась просто ледяной.\n",
        "Если помыть руки не составило особого труда, то вот умыться или прополоскать рот ей – уже большая проблема.\n",
        "В пакетике, который мне дала Ольга Дмитриевна, не нашлось зубной пасты.\n",
        "Можно, конечно, было почистить зубы и так, но в полотенце была завернута какая-то кругленькая коробочка.\n",
        "«Зубной порошок».\n",
        "Мысли: Прелестно! +1 за то, что я где-то в прошлом.\n",
        "Умылся я довольно быстро, в том числе и из-за ледяной воды.\n",
        "Кто-то быстро шёл, даже бежал в мою сторону.\n",
        "Я обернулся.\n",
        "Передо мной стояла Славя в спортивном костюме.\n",
        "Похоже, эта девочка будет хорошо выглядеть абсолютно во всём – и в пионерской форме, и в купальнике, и, наверное, даже в космическом скафандре.\n",
        "Славя: Физкульт-привет!\n",
        "Семён: Охай… То есть, бобр… Доброе утро! Вот…\n",
        "Приветствие мне удалось выбрать не сразу.\n",
        "Славя: Почему на завтрак не пришёл?\n",
        "Семён: Проспал.\n",
        "Я сказал это так, словно гордился своим достижением.\n",
        "Семён: Но мне Ольга Дмитриевна бутерброды принесла.\n",
        "Славя: А, ну отлично тогда! Не забудь про линейку!\n",
        "Семён: Да, конечно.\n",
        "Мысли: Забудешь тут.\n",
        "Славя: Ладно, я побежала, не скучай!\n",
        "Она помахала мне на прощание и скрылась за поворотом тропинки.\n",
        "Мысли: Судя по всему, линейка начнётся через пару минут.\n",
        "Стоило быстренько забежать «домой», закинуть пакетик с умывальными принадлежностями, съесть бутерброды и только уже потом идти на площадь.\n",
        "Я распахнул дверь домика вожатой и вбежал внутрь так, как будто запрыгивал в последний вагон уходящего поезда.\n",
        "Но, кажется, это было не лучшим решением – посреди комнаты стояла Ольга Дмитриевна…\n",
        "И переодевалась!\n",
        "\"\"\".replace(\"\\n\",\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(prompt)"
      ],
      "metadata": {
        "id": "3L-3e9X6GkHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26804c73-3297-4aa7-8f40-59c248ac6908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7301"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed=123\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "gen(\"Здравствуйте\",20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "U--RZUtaivKV",
        "outputId": "952c890a-2fa5-4a2b-8843-3308503a3426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Здравствуйте. помогите пожалуйста решить задачу. У меня есть 2 сестры, одна родная, а другая двоюродная.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP77MF8LxKdc"
      },
      "outputs": [],
      "source": [
        "text = prompt\n",
        "while 1:\n",
        "  text=next(text,30)\n",
        "  print(text)\n",
        "  text+=input(\"Вы:\").replace(\"@\",\"Семён:\").replace(\";;;\",\"\\n\")\n",
        "  text=text[-2000:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture muted\n",
        "!pip install auto-gptq gradio\n",
        "!git clone -b peft_integration https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n",
        "!pip install .[triton]\n",
        "%cd ..\n",
        "#!git clone https://github.com/timdettmers/bitsandbytes.git\n",
        "!pip install bitsandbytes\n",
        "!git clone https://github.com/qwopqwop200/gptqlora\n",
        "%cd gptqlora\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -r requirements.txt\n",
        "!pip install protobuf==3.20.*\n",
        "!pip install huggingface_hub\n",
        "!pip uninstall peft\n",
        "!pip install peft==0.4.0"
      ],
      "metadata": {
        "id": "92-Dv0dSpKEd"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}