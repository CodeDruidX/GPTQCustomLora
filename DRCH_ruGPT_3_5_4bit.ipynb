{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiMUa1HcvjQS"
      },
      "source": [
        "#Dear chan (DRCH novel) and my **GPTQCustomLORA**\n",
        "Main Article (RU): https://habr.com/ru/articles/751972/\n",
        "\n",
        "Creator: https://github.com/CodeDruidX\n",
        "\n",
        "Src text: https://github.com/CodeDruidX/Everlasting-Summer-txt\n",
        "\n",
        "Fork from: https://github.com/qwopqwop200/gptqlora\n",
        "\n",
        "Origin: https://github.com/artidoro/qlora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/qwopqwop200/gptqlora\n",
        "!pip install auto-gptq\n",
        "!pip install accelerate\n",
        "!pip install evaluate\n",
        "!pip install transformers==4.31.0\n",
        "!pip install peft==0.4.0"
      ],
      "metadata": {
        "id": "lgEuNClGk3FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture mutedtoo\n",
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"gurgutan/ruGPT-13B-4bit\",local_dir=r\"/ruGPT-3.5\")"
      ],
      "metadata": {
        "id": "O0segMVIor3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Your function to create datasets.Dataset\n",
        "def prepare_dataset():\n",
        "  from transformers import TextDataset\n",
        "  from datasets import Dataset\n",
        "  with open(\"/gptqlora/data.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    l=f.readlines()\n",
        "  #l=l.replace(\"\\n\",\"\\n\\n\")\n",
        "  import random\n",
        "\n",
        "  def get_random_substring(input_string, length):\n",
        "      start = random.randrange(0, len(input_string) - length + 1)\n",
        "      return input_string[start : start + length]\n",
        "  #print(get_random_substring(l,500))\n",
        "  l2=list(set([\"\\n\".join(get_random_substring(l,random.randint(4,6))) for i in range(2000)]))\n",
        "  print(len(l2),len(set(l2)))\n",
        "  l = list(map(lambda x: {\n",
        "              'input': '',\n",
        "              'output': x\n",
        "          },l2))\n",
        "  print(l2)\n",
        "  dataset=Dataset.from_list(l)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "pjRTSMUCTe2c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv=['gptqlora.py', 'â€“learning_rate', '0.0001', '--model_path', '/ruGPT-3.5', '--max_steps', '300', '--dataset', 'CUSTOM-BABY', '--per_device_train_batch_size', '1', '--logging_steps', '5', '--save_steps', '20', '--output_dir', './output2']"
      ],
      "metadata": {
        "id": "JxBo5TKbZKrv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPTQCustomLora\n",
        "# This source code is licensed under the MIT license\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists, join, isdir\n",
        "from dataclasses import dataclass, field\n",
        "import sys\n",
        "from typing import Optional, Dict, Sequence\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    LineByLineTextDataset,\n",
        "    set_seed,\n",
        "    Seq2SeqTrainer,\n",
        "    LlamaTokenizerFast\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftModel\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from auto_gptq.utils.peft_utils import get_gptq_peft_model, GPTQLoraConfig\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from auto_gptq.nn_modules.qlinear import GeneralQuantLinear\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def prepare_model_for_int8_training(model, use_gradient_checkpointing=True):\n",
        "    r\"\"\"\n",
        "    This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
        "        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n",
        "        head to fp32\n",
        "\n",
        "    Args:\n",
        "        model, (`transformers.PreTrainedModel`):\n",
        "            The loaded model from `transformers`\n",
        "    \"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        # freeze base model's layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if use_gradient_checkpointing:\n",
        "        # For backward compatibility\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        else:\n",
        "\n",
        "            def make_inputs_require_grad(module, input, output):\n",
        "                output.requires_grad_(True)\n",
        "\n",
        "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "        # enable gradient checkpointing for memory efficiency\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    return model\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_path: Optional[str] = field(\n",
        "        default=\"./llama-7b/\"\n",
        "    )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enable unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    eval_dataset_size: int = field(\n",
        "        default=1024, metadata={\"help\": \"Size of validation dataset.\"}\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    source_max_len: int = field(\n",
        "        default=1024,\n",
        "        metadata={\"help\": \"Maximum source sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    target_max_len: int = field(\n",
        "        default=256,\n",
        "        metadata={\"help\": \"Maximum target sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    dataset: str = field(\n",
        "        default='alpaca',\n",
        "        metadata={\"help\": \"Which dataset to finetune on. See datamodule for options.\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None\n",
        "    )\n",
        "    train_on_source: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to train on the input in addition to the target text.\"}\n",
        "    )\n",
        "    mmlu_split: Optional[str] = field(\n",
        "        default='eval',\n",
        "        metadata={\"help\": \"The MMLU split to run on\"}\n",
        "    )\n",
        "    mmlu_dataset: Optional[str] = field(\n",
        "        default='mmlu-fs',\n",
        "        metadata={\"help\": \"MMLU dataset to use: options are `mmlu-zs` for zero-shot or `mmlu-fs` for few shot.\"}\n",
        "    )\n",
        "    do_mmlu_eval: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to run the MMLU evaluation.\"}\n",
        "    )\n",
        "    max_mmlu_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If set, only evaluates on `max_mmlu_samples` of the MMMLU dataset.\"}\n",
        "    )\n",
        "    mmlu_source_max_len: int = field(\n",
        "        default=2048,\n",
        "        metadata={\"help\": \"Maximum source sequence length for mmlu.\"}\n",
        "    )\n",
        "    full_finetune: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Finetune the entire model without adapters.\"}\n",
        "    )\n",
        "    adam8bit: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Use 8-bit adam.\"}\n",
        "    )\n",
        "    lora_r: int = field(\n",
        "        default=64,\n",
        "        metadata={\"help\": \"Lora R dimension.\"}\n",
        "    )\n",
        "    lora_alpha: float = field(\n",
        "        default=16,\n",
        "        metadata={\"help\": \" Lora alpha.\"}\n",
        "    )\n",
        "    lora_dropout: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\"help\":\"Lora dropout.\"}\n",
        "    )\n",
        "    max_memory_MB: int = field(\n",
        "        default=24000,\n",
        "        metadata={\"help\": \"Free memory per gpu.\"}\n",
        "    )\n",
        "    report_to: str = field(\n",
        "        default='none',\n",
        "        metadata={\"help\": \"To use wandb or something else for reporting.\"}\n",
        "    )\n",
        "    output_dir: str = field(default='./output', metadata={\"help\": 'The output dir for logs and checkpoints'})\n",
        "    optim: str = field(default='paged_adamw_32bit', metadata={\"help\": 'The optimizer to be used'})\n",
        "    per_device_train_batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n",
        "    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n",
        "    max_steps: int = field(default=10000, metadata={\"help\": 'How many optimizer update steps to take'})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) # use lora dropout instead for regularization if needed\n",
        "    learning_rate: float = field(default=0.0002, metadata={\"help\": 'The learnign rate'})\n",
        "    remove_unused_columns: bool = field(default=False, metadata={\"help\": 'Removed unused columns. Needed to make this codebase work.'})\n",
        "    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n",
        "    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n",
        "    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n",
        "    lr_scheduler_type: str = field(default='constant', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n",
        "    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n",
        "    logging_steps: int = field(default=10, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n",
        "    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n",
        "    save_strategy: str = field(default='steps', metadata={\"help\": 'When to save checkpoints'})\n",
        "    save_steps: int = field(default=250, metadata={\"help\": 'How often to save a model'})\n",
        "    save_total_limit: int = field(default=40, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n",
        "\n",
        "@dataclass\n",
        "class GenerationArguments:\n",
        "    # For more hyperparameters check:\n",
        "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
        "    # Length arguments\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default=256,\n",
        "        metadata={\"help\": \"Maximum number of new tokens to be generated in evaluation or prediction loops\"\n",
        "                          \"if predict_with_generate is set.\"}\n",
        "    )\n",
        "    min_new_tokens : Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Minimum number of new tokens to generate.\"}\n",
        "    )\n",
        "\n",
        "    # Generation strategy\n",
        "    do_sample: Optional[bool] = field(default=False)\n",
        "    num_beams: Optional[int] = field(default=1)\n",
        "    num_beam_groups: Optional[int] = field(default=1)\n",
        "    penalty_alpha: Optional[float] = field(default=None)\n",
        "    use_cache: Optional[bool] = field(default=True)\n",
        "\n",
        "    # Hyperparameters for logit manipulation\n",
        "    temperature: Optional[float] = field(default=1.0)\n",
        "    top_k: Optional[int] = field(default=50)\n",
        "    top_p: Optional[float] = field(default=1.0)\n",
        "    typical_p: Optional[float] = field(default=1.0)\n",
        "    diversity_penalty: Optional[float] = field(default=0.0)\n",
        "    repetition_penalty: Optional[float] = field(default=1.0)\n",
        "    length_penalty: Optional[float] = field(default=1.0)\n",
        "    no_repeat_ngram_size: Optional[int] = field(default=0)\n",
        "\n",
        "def find_all_linear_names(args, model):\n",
        "    cls = GeneralQuantLinear if not(args.full_finetune) else torch.nn.Linear\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "class SavePeftModelCallback(transformers.TrainerCallback):\n",
        "    def save_model(self, args, state, kwargs):\n",
        "        print('Saving PEFT checkpoint...')\n",
        "        if state.best_model_checkpoint is not None:\n",
        "            checkpoint_folder = os.path.join(state.best_model_checkpoint, \"adapter_model\")\n",
        "        else:\n",
        "            checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        self.save_model(args, state, kwargs)\n",
        "        return control\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        def touch(fname, times=None):\n",
        "            with open(fname, 'a'):\n",
        "                os.utime(fname, times)\n",
        "\n",
        "        touch(join(args.output_dir, 'completed'))\n",
        "        self.save_model(args, state, kwargs)\n",
        "\n",
        "def get_accelerate_model(args, checkpoint_dir):\n",
        "\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{args.max_memory_MB}MB'\n",
        "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
        "\n",
        "    if args.full_finetune: assert args.bits in [16, 32]\n",
        "\n",
        "    print(f'loading base model {args.model_path}...')\n",
        "    model = AutoGPTQForCausalLM.from_quantized(\n",
        "        args.model_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map='auto',\n",
        "        max_memory=max_memory,\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "        inject_fused_attention = True,\n",
        "        inject_fused_mlp = False,\n",
        "        use_triton=True,\n",
        "        warmup_triton=False,\n",
        "        trainable=True\n",
        "    )\n",
        "    model.model.quantize_config = model.quantize_config\n",
        "    model.train()\n",
        "\n",
        "    setattr(model, 'model_parallel', True)\n",
        "    setattr(model, 'is_parallelizable', True)\n",
        "    #modules = find_all_linear_names(args, model)\n",
        "\n",
        "    model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
        "\n",
        "    if not args.full_finetune:\n",
        "        model = prepare_model_for_int8_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
        "    if args.gradient_checkpointing:\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    config = GPTQLoraConfig(\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        #target_modules=modules,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    if not args.full_finetune:\n",
        "        if checkpoint_dir is not None:\n",
        "            print(\"Loading adapters from checkpoint.\")\n",
        "            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'))\n",
        "            for name, p in model.named_parameters():\n",
        "                if 'lora' in name:\n",
        "                    print(name, p.sum())\n",
        "        else:\n",
        "            print(f'adding LoRA modules...')\n",
        "            model = get_gptq_peft_model(model, config, auto_find_all_linears=True, train_mode=True)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        else:\n",
        "            def make_inputs_require_grad(module, input, output):\n",
        "                output.requires_grad_(True)\n",
        "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LoraLayer):\n",
        "            if args.bf16:\n",
        "                module = module.to(torch.bfloat16)\n",
        "        if 'norm' in name:\n",
        "            module = module.to(torch.float32)\n",
        "        if 'lm_head' in name or 'embed_tokens' in name:\n",
        "            if hasattr(module, 'weight'):\n",
        "                if args.bf16 and module.weight.dtype == torch.float32:\n",
        "                    module = module.to(torch.bfloat16)\n",
        "    return model\n",
        "\n",
        "def print_trainable_parameters(args, model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    try:\n",
        "        trainable_params /= (32//model.quantize_config.bits)\n",
        "    except:\n",
        "        pass\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param}\")\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "    source_max_len: int\n",
        "    target_max_len: int\n",
        "    train_on_source: bool\n",
        "    predict_with_generate: bool\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract elements\n",
        "        sources = [example['input'] for example in instances]\n",
        "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
        "        # Tokenize\n",
        "        tokenized_sources_with_prompt = self.tokenizer(\n",
        "            sources,\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "        )\n",
        "        tokenized_targets = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        # Build the input and labels for causal LM\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "        for tokenized_source, tokenized_target in zip(\n",
        "            tokenized_sources_with_prompt['input_ids'],\n",
        "            tokenized_targets['input_ids']\n",
        "        ):\n",
        "            if not self.predict_with_generate:\n",
        "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
        "                if not self.train_on_source:\n",
        "                    labels.append(\n",
        "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
        "                    )\n",
        "                else:\n",
        "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
        "            else:\n",
        "                input_ids.append(torch.tensor(tokenized_source))\n",
        "        # Apply padding\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
        "        data_dict = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        }\n",
        "        if labels is not None:\n",
        "            data_dict['labels'] = labels\n",
        "        return data_dict\n",
        "\n",
        "def extract_unnatural_instructions_data(examples, extract_reformulations=False):\n",
        "    out = {\n",
        "        'input': [],\n",
        "        'output': [],\n",
        "    }\n",
        "    for example_instances in examples['instances']:\n",
        "        for instance in example_instances:\n",
        "            out['input'].append(instance['instruction_with_input'])\n",
        "            out['output'].append(instance['output'])\n",
        "    if extract_reformulations:\n",
        "        for example_reformulations in examples['reformulations']:\n",
        "            if example_reformulations is not None:\n",
        "                for instance in example_reformulations:\n",
        "                    out['input'].append(instance['instruction_with_input'])\n",
        "                    out['output'].append(instance['output'])\n",
        "    return out\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def extract_alpaca_dataset(example):\n",
        "    if example.get(\"input\", \"\") != \"\":\n",
        "        prompt_format = PROMPT_DICT[\"prompt_input\"]\n",
        "    else:\n",
        "        prompt_format = PROMPT_DICT[\"prompt_no_input\"]\n",
        "    return {'input': prompt_format.format(**example)}\n",
        "\n",
        "\n",
        "def get_last_checkpoint(checkpoint_dir):\n",
        "    if isdir(checkpoint_dir):\n",
        "        is_completed = exists(join(checkpoint_dir, 'completed'))\n",
        "        if is_completed: return None, True # already finished\n",
        "        max_step = 0\n",
        "        for filename in os.listdir(checkpoint_dir):\n",
        "            if isdir(join(checkpoint_dir, filename)) and filename.startswith('checkpoint'):\n",
        "                max_step = max(max_step, int(filename.replace('checkpoint-', '')))\n",
        "        if max_step == 0: return None, is_completed # training started, but no checkpoint\n",
        "        checkpoint_dir = join(checkpoint_dir, f'checkpoint-{max_step}')\n",
        "        print(f\"Found a previous checkpoint at: {checkpoint_dir}\")\n",
        "        return checkpoint_dir, is_completed # checkpoint found!\n",
        "    return None, False # first training\n",
        "\n",
        "def train():\n",
        "    hfparser = transformers.HfArgumentParser((\n",
        "        ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
        "    ))\n",
        "    model_args, data_args, training_args, generation_args, extra_args = \\\n",
        "        hfparser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
        "    training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
        "    args = argparse.Namespace(\n",
        "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
        "    )\n",
        "    print(args)\n",
        "\n",
        "    checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
        "    if completed_training:\n",
        "        print('Detected that training was already completed!')\n",
        "\n",
        "    model = get_accelerate_model(args, checkpoint_dir)\n",
        "    training_args.skip_loading_checkpoint_weights=True\n",
        "\n",
        "    resume_from_checkpoint = checkpoint_dir\n",
        "    if resume_from_checkpoint:\n",
        "        # Check the available weights and load them\n",
        "        checkpoint_name = os.path.join(\n",
        "            checkpoint_dir, \"pytorch_model.bin\"\n",
        "        )  # Full checkpoint\n",
        "        if not os.path.exists(checkpoint_name):\n",
        "            checkpoint_path = os.path.join(\n",
        "                checkpoint_dir, \"adapter_model\"\n",
        "            )\n",
        "\n",
        "            checkpoint_name = os.path.join(\n",
        "                checkpoint_path, \"adapter_model.bin\"\n",
        "            )  # only LoRA model - LoRA config above has to fit\n",
        "            resume_from_checkpoint = (\n",
        "                False  # So the trainer won't try loading its state\n",
        "            )\n",
        "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
        "        if os.path.exists(checkpoint_name):\n",
        "            print(f\"Restarting from {checkpoint_name}\")\n",
        "            adapters_weights = torch.load(checkpoint_name)\n",
        "            set_peft_model_state_dict(model, adapters_weights)\n",
        "        else:\n",
        "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    print_trainable_parameters(args, model)\n",
        "    print('loaded model')\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.model_path,\n",
        "        cache_dir=args.cache_dir,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=True,\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "    if isinstance(tokenizer, LlamaTokenizerFast):\n",
        "        # LLaMA tokenizer may not have correct special tokens set.\n",
        "        # Check and add them if missing to prevent them from being parsed into different tokens.\n",
        "        # Note that these are present in the vocabulary.\n",
        "        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
        "                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
        "                \"unk_token\": tokenizer.convert_ids_to_tokens(model.config.pad_token_id),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    if not args.full_finetune:\n",
        "        trainer.add_callback(SavePeftModelCallback)\n",
        "    if args.do_mmlu_eval:\n",
        "        if args.mmlu_dataset == 'mmlu-zs':\n",
        "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
        "                'eval': 'data/mmlu/zero_shot_mmlu_val.json',\n",
        "                'test': 'data/mmlu/zero_shot_mmlu_test.json',\n",
        "            })\n",
        "            mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
        "        # MMLU Five-shot (Eval/Test only)\n",
        "        elif args.mmlu_dataset == 'mmlu' or args.mmlu_dataset == 'mmlu-fs':\n",
        "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
        "                'eval': 'data/mmlu/five_shot_mmlu_val.json',\n",
        "                'test': 'data/mmlu/five_shot_mmlu_test.json',\n",
        "            })\n",
        "            # mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
        "        mmlu_dataset = mmlu_dataset[args.mmlu_split]\n",
        "        if args.max_mmlu_samples is not None:\n",
        "            mmlu_dataset = mmlu_dataset.select(range(args.max_mmlu_samples))\n",
        "        abcd_idx = [\n",
        "            tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n",
        "            tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n",
        "        ]\n",
        "        accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "        class MMLUEvalCallback(transformers.TrainerCallback):\n",
        "            def on_evaluate(self, args, state, control, model, **kwargs):\n",
        "                data_loader = trainer.get_eval_dataloader(mmlu_dataset)\n",
        "                source_max_len = trainer.data_collator.source_max_len\n",
        "                trainer.data_collator.source_max_len = args.mmlu_source_max_len\n",
        "                trainer.model.eval()\n",
        "                preds, refs = [], []\n",
        "                loss_mmlu = 0\n",
        "                for batch in tqdm(data_loader, total=len(data_loader)):\n",
        "                    (loss, logits, labels) = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n",
        "                    # There are two tokens, the output, and eos token.\n",
        "                    for i, logit in enumerate(logits):\n",
        "                        label_non_zero_id = (batch['labels'][i] != -100).nonzero()[0][0]\n",
        "                        logit_abcd = logit[label_non_zero_id-1][abcd_idx]\n",
        "                        preds.append(torch.argmax(logit_abcd).item())\n",
        "                    labels = labels[labels != IGNORE_INDEX].view(-1, 2)[:,0]\n",
        "                    for label in labels.tolist():\n",
        "                        if label in abcd_idx:\n",
        "                            refs += [abcd_idx.index(label)]\n",
        "\n",
        "                    loss_mmlu += loss.item()\n",
        "                # Extract results by subject.\n",
        "                results = {'mmlu_loss':loss_mmlu/len(data_loader)}\n",
        "                subject = mmlu_dataset['subject']\n",
        "                subjects = {s:{'refs':[], 'preds':[]} for s in set(subject)}\n",
        "                for s,p,r in zip(subject, preds, refs):\n",
        "                    subjects[s]['preds'].append(p)\n",
        "                    subjects[s]['refs'].append(r)\n",
        "                subject_scores = []\n",
        "                for subject in subjects:\n",
        "                    subject_score = accuracy.compute(\n",
        "                        references=subjects[subject]['refs'],\n",
        "                        predictions=subjects[subject]['preds']\n",
        "                    )['accuracy']\n",
        "                    results[f'mmlu_{args.mmlu_split}_accuracy_{subject}'] = subject_score\n",
        "                    subject_scores.append(subject_score)\n",
        "                results[f'mmlu_{args.mmlu_split}_accuracy'] = np.mean(subject_scores)\n",
        "                trainer.log(results)\n",
        "                trainer.data_collator.source_max_len = source_max_len\n",
        "\n",
        "        trainer.add_callback(MMLUEvalCallback)\n",
        "\n",
        "    # Verifying the datatypes.\n",
        "    dtypes = {}\n",
        "    for _, p in model.named_parameters():\n",
        "        dtype = p.dtype\n",
        "        if dtype not in dtypes: dtypes[dtype] = 0\n",
        "        dtypes[dtype] += p.numel()\n",
        "    total = 0\n",
        "    for k, v in dtypes.items(): total+= v\n",
        "    for k, v in dtypes.items():\n",
        "        print(k, v, v/total)\n",
        "\n",
        "    all_metrics = {\"run_name\": args.run_name}\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        all_metrics.update(metrics)\n",
        "    # Evaluation\n",
        "    if args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "        all_metrics.update(metrics)\n",
        "    # Prediction\n",
        "    if args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "        prediction_output = trainer.predict(test_dataset=data_module['predict_dataset'],metric_key_prefix=\"predict\")\n",
        "        prediction_metrics = prediction_output.metrics\n",
        "        predictions = prediction_output.predictions\n",
        "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "        predictions = tokenizer.batch_decode(\n",
        "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        with open(os.path.join(args.output_dir, 'predictions.jsonl'), 'w') as fout:\n",
        "            for i, example in enumerate(data_module['predict_dataset']):\n",
        "                example['prediction_with_input'] = predictions[i].strip()\n",
        "                example['prediction'] = predictions[i].replace(example['input'], '').strip()\n",
        "                fout.write(json.dumps(example) + '\\n')\n",
        "        print(prediction_metrics)\n",
        "        trainer.log_metrics(\"predict\", prediction_metrics)\n",
        "        trainer.save_metrics(\"predict\", prediction_metrics)\n",
        "        all_metrics.update(prediction_metrics)\n",
        "\n",
        "    if (args.do_train or args.do_eval or args.do_predict):\n",
        "        with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
        "            fout.write(json.dumps(all_metrics))\n",
        "\n",
        "\n",
        "def make_data_module(tokenizer: transformers.PreTrainedTokenizer, args) -> Dict:\n",
        "    # From origin author:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Make dataset and collator for supervised fine-tuning.\n",
        "    Datasets are expected to have the following columns: { `input`, `output` }\n",
        "\n",
        "    Available datasets to be selected with `dataset` argument:\n",
        "        - alpaca, 52002 examples\n",
        "        - alpaca cleaned, 51942 examples\n",
        "        - chip2 (OIG), 210289 examples\n",
        "        - self-instruct, 82612 examples\n",
        "        - hh-rlhf (Anthropic), 160800 examples\n",
        "        - longform, 23.7k examples\n",
        "\n",
        "    Coming soon:\n",
        "        - unnatural instructions core, 66010 examples\n",
        "        - unnatural instructions full, 240670 examples\n",
        "        - alpaca-gpt4, 52002 examples\n",
        "        - unnatural-instructions-gpt4, 9000 examples\n",
        "        - oa-rlhf (OpenAssistant) primary message tree only, 9209 examples\n",
        "        - oa-rlhf-assistant (OpenAssistant) all assistant  replies with ranking\n",
        "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
        "        - flan (FLAN v2), up to 20M examples available\n",
        "\n",
        "    Not Available:\n",
        "        - vicuna, not released at the moment.\n",
        "    \"\"\"\n",
        "    # From me:\n",
        "    \"\"\"\n",
        "    Just keep 'CUSTOM-BABY' as arg and edit prepare_dataset()\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Load dataset.\n",
        "    # Alpaca\n",
        "    print(args.dataset)\n",
        "\n",
        "    if args.dataset == 'CUSTOM-BABY':\n",
        "      dataset=prepare_dataset()\n",
        "      print(dataset)\n",
        "      #train_dataset=LineByLineTextDataset(tokenizer=tokenizer,file_path=\"/gptqlora/datafromES.txt\",block_size=64)\n",
        "      #print(train_dataset.examples)\n",
        "      #train_dataset.examples = list(map(lambda x: {\n",
        "      #      'input': '',\n",
        "      #      'output': x\n",
        "      #  },train_dataset.examples))\n",
        "    #elif args.dataset == 'alpaca':\n",
        "    # Alpaca clean\n",
        "    elif args.dataset == 'alpaca-clean':\n",
        "        dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
        "        dataset = dataset.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
        "    # Chip2\n",
        "    elif args.dataset == 'chip2':\n",
        "        dataset = load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
        "        dataset = dataset.map(lambda x: {\n",
        "            'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
        "            'output': x['text'].split('\\n<bot>: ')[1],\n",
        "        }, remove_columns=['text', 'metadata'])\n",
        "    # Self Instruct\n",
        "    elif args.dataset == 'self-instruct':\n",
        "        dataset = load_dataset(\"yizhongw/self_instruct\", name='self_instruct')\n",
        "        for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
        "            dataset = dataset.rename_column(old, new)\n",
        "    # Anthropic rlhf\n",
        "    elif args.dataset == 'hh-rlhf':\n",
        "        dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
        "        dataset = dataset.map(lambda x: {\n",
        "            'input': '',\n",
        "            'output': x['chosen']\n",
        "        }, remove_columns=['chosen', 'rejected'])\n",
        "        print(dataset)\n",
        "    # LongForm\n",
        "    elif args.dataset == 'longform':\n",
        "        dataset = load_dataset(\"akoksal/LongForm\")\n",
        "    elif args.dataset == 'vicuna':\n",
        "        raise NotImplementedError(\"Vicuna data was not released.\")\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Dataset {args.dataset} not implemented yet.\")\n",
        "\n",
        "    # Split train/eval, reduce size\n",
        "    if args.do_eval or args.do_predict:\n",
        "        if 'eval' in dataset:\n",
        "            eval_dataset = dataset['eval']\n",
        "        else:\n",
        "            print('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
        "            dataset = dataset[\"train\"].train_test_split(\n",
        "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
        "            )\n",
        "            eval_dataset = dataset['test']\n",
        "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
        "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
        "        if args.group_by_length:\n",
        "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "    if args.do_train:\n",
        "        #train_dataset = dataset['train']\n",
        "        train_dataset = dataset\n",
        "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
        "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "        if args.group_by_length:\n",
        "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "\n",
        "    data_collator = DataCollatorForCausalLM(\n",
        "        tokenizer=tokenizer,\n",
        "        source_max_len=args.source_max_len,\n",
        "        target_max_len=args.target_max_len,\n",
        "        train_on_source=args.train_on_source,\n",
        "        predict_with_generate=args.predict_with_generate,\n",
        "    )\n",
        "    return dict(\n",
        "        train_dataset=train_dataset if args.do_train else None,\n",
        "        eval_dataset=eval_dataset if args.do_eval else None,\n",
        "        predict_dataset=eval_dataset if args.do_predict else None,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WP8ETHl5Ro7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# ALL NEXT CODE IS NOT MATTER\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "7313uDsJUyyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJNzaJbeWWhB",
        "outputId": "8291f164-5daa-4ff3-cc28-57a8d4d6ae5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_N8D_Fvwf5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test model with lora\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import auto_gptq\n",
        "from auto_gptq import AutoGPTQForCausalLM, get_gptq_peft_model\n",
        "from auto_gptq.utils.peft_utils import GPTQLoraConfig\n",
        "#import gradio as gr\n",
        "\n",
        "model_name = 'fffrrt/ruGPT-3.5-13B-GPTQ'\n",
        "model_basename = 'gptq_model-4bit-128g'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoGPTQForCausalLM.from_quantized(\"gurgutan/ruGPT-13B-4bit\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "        inject_fused_attention = True,\n",
        "        inject_fused_mlp = False,\n",
        "        use_triton=True,\n",
        "        warmup_triton=False,\n",
        "        trainable=True)\n",
        "\n",
        "#t=torch.load('saved_model.pth')\n",
        "#model.load_state_dict(t['model_state_dict'])\n",
        "# loaded_tokenizer = t['tokenizer']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gptqlora.py -â€“learning_rate 0.0001 --model_path /ruGPT-3.5 --max_steps 300 --dataset CUSTOM-BABY --per_device_train_batch_size 1 --logging_steps 5 --save_steps 20 --output_dir ./output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1oNcZJk4US-",
        "outputId": "22593d0c-a5e0-46fa-b191-a8b3d51038a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-28 14:48:52.414767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading base model /ruGPT-3.5...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 672, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 164, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '/ruGPT-3.5'.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 792, in <module>\n",
            "    train()\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 601, in train\n",
            "    model = get_accelerate_model(args, checkpoint_dir)\n",
            "  File \"/content/gptqlora/gptqlora.py\", line 282, in get_accelerate_model\n",
            "    model = AutoGPTQForCausalLM.from_quantized(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py\", line 87, in from_quantized\n",
            "    model_type = check_and_get_model_type(model_name_or_path, trust_remote_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py\", line 147, in check_and_get_model_type\n",
            "    config = AutoConfig.from_pretrained(model_dir, trust_remote_code=trust_remote_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 983, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 617, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 693, in _get_config_dict\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load the configuration of '/ruGPT-3.5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/ruGPT-3.5' is the correct path to a directory containing a config.json file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = GPTQLoraConfig(\n",
        "    inference_mode=True,\n",
        ")\n",
        "model = get_gptq_peft_model(model, peft_config, '/content/drive/MyDrive/ckp/adapter_model/adapter_model')"
      ],
      "metadata": {
        "id": "wjv6Dlrjrp3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.is_loaded_in_4bit=True"
      ],
      "metadata": {
        "id": "0Kj_IL4KPYNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=torch.load('saved_model.pth',torch.device('cpu'))\n",
        "model.load_state_dict(t['model_state_dict'])\n",
        "tokenizer = t['tokenizer']\n",
        "\n",
        "del t\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrhA36FR7tA7",
        "outputId": "de97cf2b-0741-4871-99ab-a166ab8c6301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg={\n",
        "\"K\":40,\n",
        "\"P\":0.98,\n",
        "\"temperature\":0.6,\n",
        "\"uningram\":4\n",
        "}\n",
        "def editcfg(name,val):\n",
        "  print(name,val)\n",
        "  global cfg\n",
        "  cfg[name]=val\n",
        "\n",
        "def gen(text,tokens=10):\n",
        "  encoded_input = tokenizer(text, return_tensors='pt').to('cuda:0')\n",
        "  print(encoded_input.input_ids[:,-1900:].shape)\n",
        "  output = model.generate(\n",
        "      input_ids=encoded_input.input_ids[:,-1900:],\n",
        "      max_new_tokens=tokens,\n",
        "      do_sample=True,\n",
        "      top_k=cfg['K'],\n",
        "      top_p=cfg['P'],\n",
        "      temperature=cfg['temperature'],\n",
        "      #num_beams=4,\n",
        "      no_repeat_ngram_size=cfg['uningram'],\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      # num_return_sequences=5,\n",
        "      # do_sample=True\n",
        "      #repetition_penalty = 1.04\n",
        "  )\n",
        "\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "GHiktwGF9pKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oe8rQWd20QC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f930959-01ef-43b7-8b05-81643edf6cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n",
            "torch.Size([1, 277])\n",
            "torch.Size([1, 307])\n",
            "torch.Size([1, 337])\n",
            "torch.Size([1, 367])\n",
            "torch.Size([1, 397])\n",
            "torch.Size([1, 279])\n",
            "torch.Size([1, 309])\n",
            "torch.Size([1, 339])\n",
            "torch.Size([1, 369])\n",
            "torch.Size([1, 399])\n",
            "K 40\n",
            "P 0.98\n",
            "uningram 4\n",
            "temperature 0.2\n",
            "torch.Size([1, 22])\n",
            "torch.Size([1, 52])\n",
            "torch.Size([1, 28])\n",
            "torch.Size([1, 53])\n",
            "torch.Size([1, 83])\n",
            "torch.Size([1, 113])\n",
            "torch.Size([1, 143])\n"
          ]
        }
      ],
      "source": [
        "#@title UI\n",
        "import gradio as gr\n",
        "def complete_with_gpt(text):\n",
        "    text=gen(text,30)\n",
        "    return text\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Ever novel\")\n",
        "    textbox = gr.Textbox(placeholder=\"Ð’ÐµÑ€Ð½Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ð¼ÐµÑÑ‚Ð¾!\", value=\"123\")\n",
        "    btn = gr.Button(\"ÐžÐ¶Ð¸Ð´Ð°Ñ‚ÑŒ\")\n",
        "    gr.Markdown(\"---\")\n",
        "    with gr.Row():\n",
        "      K=gr.Number(label=\"top_K\",value=cfg[\"K\"],minimum=1)\n",
        "      P=gr.Number(label=\"top_P\",value=cfg[\"P\"],minimum=0,maximum=1)\n",
        "      T=gr.Number(label=\"Temperature\",value=cfg[\"temperature\"],minimum=0)\n",
        "      U=gr.Number(label=\"Unique ngram len\",value=cfg[\"uningram\"],minimum=0)\n",
        "      APPLY=gr.Button(value=\"ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ\")\n",
        "\n",
        "    APPLY.click(fn=lambda x: editcfg(\"K\",int(x)),inputs=K)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"P\",x),inputs=P)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"temperature\",x),inputs=T)\n",
        "    APPLY.click(fn=lambda x: editcfg(\"uningram\",int(x)),inputs=U)\n",
        "    btn.click(fn=complete_with_gpt, inputs=textbox, outputs=textbox, queue=False)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install peft\n",
        "#!pip install bitsandbytes\n",
        "#del peft\n",
        "from importlib import reload\n",
        "import peft\n",
        "peft=reload(peft)\n",
        "import bitsandbytes as bnb\n",
        "#from peft import LoraConfig, get_peft_model,  prepare_model_for_kbit_training\n",
        "import torch\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "config = peft.LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = peft.get_peft_model(model, config)\n",
        "\n",
        "model = peft.prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ð¼ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² .txt Ñ„Ð°Ð¹Ð»\n",
        "train_path = 'train_dataset.txt'\n",
        "#with open(train_path, \"w\") as f:\n",
        "#    f.write(prompt)\n",
        "\n",
        "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=64)\n",
        "\n",
        "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°Ð»Ð¾Ð´ÐµÑ€Ð° (Ð½Ð°Ñ€ÐµÐ·Ð°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾ Ð´Ð»Ð¸Ð½Ðµ ÐºÑƒÑÐºÐ¸)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
        "                                                mlm=False)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned2\", # The output directory\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output dir\n",
        "    num_train_epochs=0.01, # number of training epochs\n",
        "    per_device_train_batch_size=1, # batch size for training\n",
        "    auto_find_batch_size=True,\n",
        "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
        "    warmup_steps=10, # number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=1, # to make \"virtual\" batch size larger\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
        ")\n",
        "trainer.train()\n",
        "torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'tokenizer': tokenizer\n",
        "  }, 'saved_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "RdwvgGrxRmBy",
        "outputId": "61e9d90c-7b70-49a6-aab8-3e41edd440e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dd635c94f2db>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/mapping.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptLearningConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_prompt_learning_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptLearningConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# transformers models have a .config attribute, whose presence is assumed later on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36madd_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_and_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             raise ValueError(\n\u001b[1;32m    196\u001b[0m                 \u001b[0;34m\"LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m_find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mnew_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_target_modules_in_base_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py\u001b[0m in \u001b[0;36m_create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mloaded_in_4bit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bnb_4bit_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear4bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mfourbit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             fourbit_kwargs.update(\n\u001b[1;32m    265\u001b[0m                 {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bnb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = trainer.quantize()"
      ],
      "metadata": {
        "id": "DgOXolJpGHAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen('''Ð¡ÐµÐ¼Ñ‘Ð½: ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸, Ð¾Ð½ ÑÐ¾Ð²ÑÐµÐ¼ Ð½Ðµ ÑÑ‚Ñ€Ð°ÑˆÐ½Ñ‹Ð¹.\n",
        "Ð›ÐµÐ½Ð° Ð²Ñ‹Ð³Ð»ÑÐ½ÑƒÐ»Ð° Ñƒ Ð¼ÐµÐ½Ñ Ð¸Ð·-Ð·Ð° ÑÐ¿Ð¸Ð½Ñ‹.\n",
        "Ð›ÐµÐ½Ð°: ÐÐµ ÑÑ‚Ñ€Ð°ÑˆÐ½Ñ‹Ð¹â€¦\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð¡ÐµÐ¹Ñ‡Ð°Ñ, Ð¿Ð¾Ð´Ð¾Ð¶Ð´Ð¸.\n",
        "Ð¯ Ð¼ÑÐ³ÐºÐ¾ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð´Ð¸Ð»ÑÑ Ð¾Ñ‚ ÐµÑ‘ Ð¾Ð±ÑŠÑÑ‚Ð¸Ð¹ Ð¸ Ð¿Ð¾Ð´Ð¾ÑˆÑ‘Ð» Ðº ÑÐ¾Ð²Ñ‘Ð½ÐºÑƒ.\n",
        "Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÐºÐ°Ð·Ð°Ð»Ð¾ÑÑŒ, Ñ‡Ñ‚Ð¾ Ð¾Ð½ Ð¸ÑÐ¿ÑƒÐ³Ð°ÐµÑ‚ÑÑ Ð¸ ÑƒÐ»ÐµÑ‚Ð¸Ñ‚, Ð²Ñ‹Ð¿ÑƒÑÑ‚Ð¸Ð² Ð²Ð¾Ð»Ð°Ð½Ñ‡Ð¸Ðº.\n",
        "ÐžÐ´Ð½Ð°ÐºÐ¾ ÑÐ¾Ð²Ñ‘Ð½Ð¾Ðº, Ð¿Ð¾Ñ…Ð¾Ð¶Ðµ, Ð¸ Ð½Ðµ ÑÐ¾Ð±Ð¸Ñ€Ð°Ð»ÑÑ Ð´Ð²Ð¸Ð³Ð°Ñ‚ÑŒÑÑ Ñ Ð¼ÐµÑÑ‚Ð°.\n",
        "ÐœÐ½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÑ…Ð²Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð²Ð¾Ð»Ð°Ð½Ñ‡Ð¸Ðº Ð¸ Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ ÐµÐ³Ð¾ Ñƒ Ð¿Ñ‚Ð¸Ñ†Ñ‹.\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð¡Ð¼Ð¾Ñ‚Ñ€Ð¸, Ð¾Ð½ ÑÐ¾Ð²ÑÐµÐ¼ Ñ€ÑƒÑ‡Ð½Ð¾Ð¹! Ð¥Ð¾Ñ‡ÐµÑˆÑŒ ÐµÐ³Ð¾ Ð¿Ð¾Ð³Ð»Ð°Ð´Ð¸Ñ‚ÑŒ?\n",
        "Ð›ÐµÐ½ÐºÐ° Ð²Ð·ÑÐ»Ð° Ð¼ÐµÐ½Ñ Ð·Ð° Ñ€ÑƒÐºÑƒ.\n",
        "ÐœÑ‹ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ð»Ð¸ÑÑŒ Ðº ÑÐ¾Ð²Ñ‘Ð½ÐºÑƒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹'''.replace(\"\\n\",\"\\n\\n\"),20)"
      ],
      "metadata": {
        "id": "yFDP10C5pf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "96d48659-ba65-4014-a727-5229025baf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 142])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ð¡ÐµÐ¼Ñ‘Ð½: ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸, Ð¾Ð½ ÑÐ¾Ð²ÑÐµÐ¼ Ð½Ðµ ÑÑ‚Ñ€Ð°ÑˆÐ½Ñ‹Ð¹.\\n\\nÐ›ÐµÐ½Ð° Ð²Ñ‹Ð³Ð»ÑÐ½ÑƒÐ»Ð° Ñƒ Ð¼ÐµÐ½Ñ Ð¸Ð·-Ð·Ð° ÑÐ¿Ð¸Ð½Ñ‹.\\n\\nÐ›ÐµÐ½Ð°: ÐÐµ ÑÑ‚Ñ€Ð°ÑˆÐ½Ñ‹Ð¹â€¦\\n\\nÐ¡ÐµÐ¼Ñ‘Ð½: Ð¡ÐµÐ¹Ñ‡Ð°Ñ, Ð¿Ð¾Ð´Ð¾Ð¶Ð´Ð¸.\\n\\nÐ¯ Ð¼ÑÐ³ÐºÐ¾ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð´Ð¸Ð»ÑÑ Ð¾Ñ‚ ÐµÑ‘ Ð¾Ð±ÑŠÑÑ‚Ð¸Ð¹ Ð¸ Ð¿Ð¾Ð´Ð¾ÑˆÑ‘Ð» Ðº ÑÐ¾Ð²Ñ‘Ð½ÐºÑƒ.\\n\\nÐ¡Ð½Ð°Ñ‡Ð°Ð»Ð° ÐºÐ°Ð·Ð°Ð»Ð¾ÑÑŒ, Ñ‡Ñ‚Ð¾ Ð¾Ð½ Ð¸ÑÐ¿ÑƒÐ³Ð°ÐµÑ‚ÑÑ Ð¸ ÑƒÐ»ÐµÑ‚Ð¸Ñ‚, Ð²Ñ‹Ð¿ÑƒÑÑ‚Ð¸Ð² Ð²Ð¾Ð»Ð°Ð½Ñ‡Ð¸Ðº.\\n\\nÐžÐ´Ð½Ð°ÐºÐ¾ ÑÐ¾Ð²Ñ‘Ð½Ð¾Ðº, Ð¿Ð¾Ñ…Ð¾Ð¶Ðµ, Ð¸ Ð½Ðµ ÑÐ¾Ð±Ð¸Ñ€Ð°Ð»ÑÑ Ð´Ð²Ð¸Ð³Ð°Ñ‚ÑŒÑÑ Ñ Ð¼ÐµÑÑ‚Ð°.\\n\\nÐœÐ½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÑ…Ð²Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð²Ð¾Ð»Ð°Ð½Ñ‡Ð¸Ðº Ð¸ Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ ÐµÐ³Ð¾ Ñƒ Ð¿Ñ‚Ð¸Ñ†Ñ‹.\\n\\nÐ¡ÐµÐ¼Ñ‘Ð½: Ð¡Ð¼Ð¾Ñ‚Ñ€Ð¸, Ð¾Ð½ ÑÐ¾Ð²ÑÐµÐ¼ Ñ€ÑƒÑ‡Ð½Ð¾Ð¹! Ð¥Ð¾Ñ‡ÐµÑˆÑŒ ÐµÐ³Ð¾ Ð¿Ð¾Ð³Ð»Ð°Ð´Ð¸Ñ‚ÑŒ?\\n\\nÐ›ÐµÐ½ÐºÐ° Ð²Ð·ÑÐ»Ð° Ð¼ÐµÐ½Ñ Ð·Ð° Ñ€ÑƒÐºÑƒ.\\n\\nÐœÑ‹ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ð¾ Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ð»Ð¸ÑÑŒ Ðº ÑÐ¾Ð²Ñ‘Ð½ÐºÑƒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð² ÑÑ‚Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð°Ñ‡Ð°Ð» ÐºÐ»ÐµÐ²Ð°Ñ‚ÑŒ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ñƒ ÑÐµÐ±Ñ Ð¿Ð¾Ð´ Ð½Ð¾Ð³Ð°Ð¼Ð¸.\\n\\nÐ”Ð¾Ð¹Ð´Ñ Ð´Ð¾ Ð½ÐµÐ³Ð¾'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_load_from_checkpoint(\"12123123\")"
      ],
      "metadata": {
        "id": "n9rSRXbhbJy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(\"\"\"from collections import defau\"\"\",20)"
      ],
      "metadata": {
        "id": "QwfeH0NwFIFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6d399410-97b7-4f72-8396-46f105f6fbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from collections import defauxt\\n This source code is licensed under the MIT license found in the\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Wvuziftlukg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./trained_model3/',use_safetensors=True)\n",
        "tokenizer.save_pretrained('./trained_model3/')"
      ],
      "metadata": {
        "id": "7lrBoRGDkryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo swapon /newswap"
      ],
      "metadata": {
        "id": "vGKWK999UOoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6vQZjR1irClJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()print(gen(prompt,30))\n"
      ],
      "metadata": {
        "id": "buVDt27B2Pyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
        "print(encoded_input.input_ids.shape)\n",
        "#print(len(encoded_input[0]))"
      ],
      "metadata": {
        "id": "ISZHSx5BGup7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UfrFZeN6mi4"
      },
      "outputs": [],
      "source": [
        "prompt=\"\"\"ÐœÐ½Ðµ ÑÐ½Ð¸Ð»ÑÑ ÑÐ¾Ð½â€¦\n",
        "ÐšÐ°Ð·Ð°Ð»Ð¾ÑÑŒ, Ñ Ð½Ð°Ñ…Ð¾Ð¶ÑƒÑÑŒ Ð² ÐºÐ°ÐºÐ¾Ð¼-Ñ‚Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼Ðµ, Ð° Ð²Ð¾ÐºÑ€ÑƒÐ³ â€“ Ð¿ÑƒÑÑ‚Ð¾Ñ‚Ð°.\n",
        "ÐÐ¾ Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ _Ð²Ð¾ÐºÑ€ÑƒÐ³_ â€“ Ð¯ ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾ Ð²Ð¾ Ð’ÑÐµÐ»ÐµÐ½Ð½Ð¾Ð¹.\n",
        "ÐšÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ð¾Ð½Ð° Ð²ÐµÑ€Ð½ÑƒÐ»Ð°ÑÑŒ Ð² Ð½ÐµÐºÐ¾Ðµ ÑÐ¸Ð½Ð³ÑƒÐ»ÑÑ€Ð½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¿ÐµÑ€ÐµÐ´ ÑÐ°Ð¼Ñ‹Ð¼ Ð‘Ð¾Ð»ÑŒÑˆÐ¸Ð¼ Ð’Ð·Ñ€Ñ‹Ð²Ð¾Ð¼.\n",
        "Ð˜ Ð²Ð¾Ñ‚-Ð²Ð¾Ñ‚ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ð»Ð¾ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾Ð¹Ñ‚Ð¸.\n",
        "Ð’Ð´Ñ€ÑƒÐ³ Ñ Ð½Ð°Ñ‡Ð°Ð» ÑÐ»Ñ‹ÑˆÐ°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ.\n",
        "Ð¡Ð»Ð¾Ð² Ð½Ðµ Ñ€Ð°Ð·Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ, Ð½Ð¾ Ð¾Ð½ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»ÑÑ Ð¼Ð½Ðµ Ð·Ð½Ð°ÐºÐ¾Ð¼Ñ‹Ð¼.\n",
        "Ð“Ð¾Ð»Ð¾Ñ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð½ÐµÐ¶Ð½Ð¾ Ð½Ð°ÑˆÑ‘Ð¿Ñ‚Ñ‹Ð²Ð°Ð», ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ ÑƒÐ±Ð°ÑŽÐºÐ¸Ð²Ð°Ñ.\n",
        "Ð˜ Ñ‚ÑƒÑ‚ Ñ Ð¿Ð¾Ð½ÑÐ»... Ð­Ñ‚Ð¾ Ð±Ñ‹Ð» Ð³Ð¾Ð»Ð¾Ñ Ñ‚Ð¾Ð¹ ÐÐµÐ·Ð½Ð°ÐºÐ¾Ð¼ÐºÐ¸ Ð¸Ð· Ð°Ð²Ñ‚Ð¾Ð±ÑƒÑÐ°. Ð¢Ð¾Ð¹ Ð´ÐµÐ²ÑƒÑˆÐºÐ¸ Ð¸Ð· ÑÐ½Ð°.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐÐ¾ Ñ‡Ñ‚Ð¾ Ð¾Ð½Ð° Ñ…Ð¾Ñ‡ÐµÑ‚ Ð¼Ð½Ðµ ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ? ÐšÑ‚Ð¾ Ð¾Ð½Ð°?..\n",
        "Ð¯ Ð¿Ñ€Ð¾ÑÐ½ÑƒÐ»ÑÑ.\n",
        "Ð¯Ñ€ÐºÐ¸Ðµ Ð»ÑƒÑ‡Ð¸ ÑÐ¾Ð»Ð½Ñ†Ð° Ð±Ð¸Ð»Ð¸ Ð² Ð³Ð»Ð°Ð·Ð°.\n",
        "Ð’Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð¶Ð°Ð»Ð¾ÑÑŒ Ðº Ð¿Ð¾Ð»ÑƒÐ´Ð½ÑŽ.\n",
        "Ð›ÐµÐ½Ð¸Ð²Ð¾ Ð¿Ð¾Ñ‚ÑÐ½ÑƒÐ²ÑˆÐ¸ÑÑŒ Ð¸ Ð·ÐµÐ²Ð½ÑƒÐ², Ñ Ð½Ð°Ñ‡Ð°Ð» Ð²ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ñ‚ÑŒ Ð²Ñ‡ÐµÑ€Ð°ÑˆÐ½Ð¸Ð¹ Ð´ÐµÐ½ÑŒ.\n",
        "Ð—Ð° Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐµÐºÑƒÐ½Ð´ Ð²ÑÐµ ÐµÐ³Ð¾ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¿Ñ€Ð¾Ð½ÐµÑÐ»Ð¸ÑÑŒ Ð¿ÐµÑ€ÐµÐ´ Ð³Ð»Ð°Ð·Ð°Ð¼Ð¸: Ð°Ð²Ñ‚Ð¾Ð±ÑƒÑ, Ð»Ð°Ð³ÐµÑ€ÑŒ, Ð¼ÐµÑÑ‚Ð½Ñ‹Ðµ Ð¾Ð±Ð¸Ñ‚Ð°Ñ‚ÐµÐ»Ð¸.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐÐ¾ Ð²ÐµÐ´ÑŒ Ð²ÑÑ‘ Ð½Ðµ Ñ‚Ð°Ðº, Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾!\n",
        "ÐÐµ ÑÑ‚Ð° ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ñ, Ð½Ðµ Ð¼Ð¾Ñ‘ Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ â€“ Ð¾Ð½Ð¸ Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹ Ð°Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸, â€“ Ð° Ð¼Ð¾Ñ‘ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ Ðº Ð½Ð¸Ð¼.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð’ÐµÐ´ÑŒ Ñ Ð²Ñ‡ÐµÑ€Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð·Ð°ÑÐ½ÑƒÐ» Ð·Ð´ÐµÑÑŒ, Ð° Ð´Ð¾ ÑÑ‚Ð¾Ð³Ð¾ Ð¼Ð¸Ð»Ð¾ Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð» Ñ Ð¼ÐµÑÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¸Ð¾Ð½ÐµÑ€Ð°Ð¼Ð¸, Ð´Ð°Ð¶Ðµ ÑƒÐ¼ÑƒÐ´Ñ€ÑÐ»ÑÑ ÑˆÑƒÑ‚Ð¸Ñ‚ÑŒ!\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐÐ¾ ÐºÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐµÐ±Ñ Ñ‚Ð°Ðº Ð²ÐµÑÑ‚Ð¸ Ð² Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð¾Ð¹ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸?!\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð¯ Ð¶Ðµ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ð¾ÑÑ‚ÑŒÑÑ, ÑˆÐ°Ñ€Ð°Ñ…Ð°Ñ‚ÑŒÑÑ Ð¾Ñ‚ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑˆÐ¾Ñ€Ð¾Ñ…Ð°, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ‚ÑŒ Ð»ÑŽÐ±Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð° Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð²Ñ€Ð°Ð¶Ð´ÐµÐ±Ð½Ñ‹Ð¼Ð¸ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð°Ð¼Ð¸.\n",
        "Ð’ÑÐµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¿Ñ€Ð¾ÑˆÐµÐ´ÑˆÐµÐ³Ð¾ Ð´Ð½Ñ ÑÐ»Ð¾Ð²Ð½Ð¾ Ð·Ð°Ð²Ð¾Ð»Ð¾ÐºÐ»Ð¾ Ð¿Ð¾Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð´Ñ‹Ð¼ÐºÐ¾Ð¹.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð­Ñ‚Ð¾ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¿Ð¾Ñ…Ð¾Ð¶Ðµ Ð½Ð° ÑƒÑ‚Ñ€Ð¾ Ð¿Ð¾ÑÐ»Ðµ Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐ¹ Ð¿ÑŒÑÐ½ÐºÐ¸: Ð²Ñ‡ÐµÑ€Ð°ÑˆÐ½ÐµÐµ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ, Ð½ÐµÐ¿Ñ€ÐµÐ´Ð¾ÑÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ, Ð² Ð²Ñ‹ÑÑˆÐµÐ¹ ÑÑ‚ÐµÐ¿ÐµÐ½Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ ÑƒÑ‚Ñ€Ð¾Ð¼ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ ÐºÐ¾ÑˆÐ¼Ð°Ñ€Ð¾Ð¼, Ð³Ñ€Ð¾Ñ‚ÐµÑÐºÐ½Ð¾Ð¹ Ð³Ñ€Ð°Ð²ÑŽÑ€Ð¾Ð¹ Ð¸Ð· Ð¸Ð»Ð»ÑŽÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¹ Ðº Â«Ð‘Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð¼ÐµÐ´Ð¸Ð¸Â».\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð”Ð°, Ð²ÑÑ‘ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ñ‚Ð°Ðº, Ð¾Ð´Ð½Ð°ÐºÐ¾ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð³Ð¾ ÑƒÐ¶Ðµ Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ.\n",
        "Ð¥Ð¾Ñ‚Ñ, Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ð¾Ñ†ÐµÐ½Ð¸Ð² Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÑƒ, Ñ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ð» Ð¿Ð¾ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸.\n",
        "Ð¯ Ð¾Ð³Ð»ÑÐ´ÐµÐ»ÑÑ Ð¿Ð¾ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ð°Ð¼, Ð¿Ñ‹Ñ‚Ð°ÑÑÑŒ Ð¿Ð¾Ð½ÑÑ‚ÑŒ, Ð½Ðµ Ð·Ð°Ð±Ñ€Ð¾ÑÐ¸Ð»Ð¾ Ð»Ð¸ Ð¼ÐµÐ½Ñ ÐºÑƒÐ´Ð°-Ð½Ð¸Ð±ÑƒÐ´ÑŒ ÐµÑ‰Ñ‘, Ð½Ð¾ Ð´Ð¾Ð¼Ð¸Ðº ÐžÐ»ÑŒÐ³Ð¸ Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ñ‹ Ð²Ñ‹Ð³Ð»ÑÐ´ÐµÐ» Ñ‚Ð°Ðº Ð¶Ðµ, ÐºÐ°Ðº Ð¸ Ð²Ñ‡ÐµÑ€Ð°.\n",
        "Ð’ÑÑ‘ Ð±Ñ‹Ð»Ð¾ ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ð½Ð° ÑÐ²Ð¾Ð¸Ñ… Ð¼ÐµÑÑ‚Ð°Ñ…, Ñ€Ð°Ð·Ð²Ðµ Ñ‡Ñ‚Ð¾ Ð½Ð° ÑÐ¿Ð¸Ð½ÐºÐµ ÐºÑ€Ð¾Ð²Ð°Ñ‚Ð¸ Ð²Ð¸ÑÐµÐ»Ð° Ð¿Ð¸Ð¾Ð½ÐµÑ€ÑÐºÐ°Ñ Ñ„Ð¾Ñ€Ð¼Ð°.\n",
        "Ð¯ Ñ Ð½ÐµÐ´Ð¾Ð²ÐµÑ€Ð¸ÐµÐ¼ Ð¿Ð¾ÐºÑ€ÑƒÑ‚Ð¸Ð» ÐµÑ‘ Ð² Ñ€ÑƒÐºÐ°Ñ…, Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¸Ð» Ð¸ Ð¾Ð´ÐµÐ»ÑÑ.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð’ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ ÑÑ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐµ, Ñ‡ÐµÐ¼ Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð² Ð·Ð¸Ð¼Ð½ÐµÐ¹ Ð¾Ð´ÐµÐ¶Ð´Ðµ.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð±Ñ‹ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð½Ð° ÑÐµÐ±Ñ â€“ Ð½Ð°Ð²ÐµÑ€Ð½ÑÐºÐ° Ð²Ñ‹Ð³Ð»ÑÐ¶Ñƒ ÐºÐ°Ðº ÐºÐ»Ð¾ÑƒÐ½!\n",
        "Ð Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð·ÐµÑ€ÐºÐ°Ð»Ð¾. Ð¥Ð¾Ñ‚Ñ Ð±Ñ‹ ÑÐ°Ð¼Ð¾Ðµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¾Ðµ.\n",
        "ÐÐ°ÑˆÐ»Ð¾ÑÑŒ Ð¾Ð½Ð¾ Ð½Ð° Ð´Ð²ÐµÑ€Ñ†Ðµ ÑˆÐºÐ°Ñ„Ð°.\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð¢Ð²Ð¾ÑŽ!..\n",
        "Ð¯ Ð²Ð·Ð³Ð»ÑÐ½ÑƒÐ» Ð½Ð° Ð½Ð¾Ð²Ð¾Ð¸ÑÐ¿ÐµÑ‡Ñ‘Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¸Ð¾Ð½ÐµÑ€Ð° Ð¸ Ð°Ð¶ Ð¾Ñ‚Ð¿Ñ€Ñ‹Ð³Ð½ÑƒÐ» Ð² ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ Ð¾Ñ‚ Ð½ÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸!\n",
        "ÐÐ° Ð´Ñ€ÑƒÐ³Ð¾Ð¹ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ðµ Ð·ÐµÑ€ÐºÐ°Ð»Ð° ÑÑ‚Ð¾ÑÐ» ÐºÐ°ÐºÐ¾Ð¹-Ñ‚Ð¾ Ð¿Ð¾Ð´Ñ€Ð¾ÑÑ‚Ð¾Ðº!\n",
        "ÐŸÐ¾Ñ…Ð¾Ð¶Ð¸Ð¹ Ð½Ð° Ð¼ÐµÐ½Ñ, Ð½Ð¾ Ð½Ðµ Ñ!\n",
        "ÐšÑƒÐ´Ð° Ð¿Ñ€Ð¾Ð¿Ð°Ð»Ð¸ Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð°Ñ Ð½ÐµÐ±Ñ€Ð¸Ñ‚Ð¾ÑÑ‚ÑŒ, Ð¼ÐµÑˆÐºÐ¸ Ð¿Ð¾Ð´ Ð³Ð»Ð°Ð·Ð°Ð¼Ð¸, ÑÑƒÑ‚ÑƒÐ»Ð¾ÑÑ‚ÑŒ Ð¸ ÑÐ¼ÐµÑ€Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÑÑ‚Ð°Ð²ÑˆÐµÐµ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð»Ð¸Ñ†Ð°?!\n",
        "ÐŸÐ¾Ñ…Ð¾Ð¶Ðµ, Ð¼ÐµÐ½Ñ Ð½Ðµ Ð·Ð°ÐºÐ¸Ð½ÑƒÐ»Ð¸ Ð½Ð°Ð·Ð°Ð´ Ð²Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸Ð»Ð¸ Ð² Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½ÑƒÑŽ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ, Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ð¾Ð¼ÐµÐ½ÑÐ»Ð¸ Ñ ÐºÐµÐ¼-Ñ‚Ð¾ Ñ‚ÐµÐ»Ð°Ð¼Ð¸.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð”ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾! Ð¢Ð°ÐºÐ¾Ðµ Ð¶Ðµ Ð½Ð° ÐºÐ°Ð¶Ð´Ð¾Ð¼ ÑˆÐ°Ð³Ñƒ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ!\n",
        "Ð¯ Ð¿Ñ€Ð¸Ð³Ð»ÑÐ´ÐµÐ»ÑÑ Ðº Ð½ÐµÐ·Ð½Ð°ÐºÐ¾Ð¼Ñ†Ñƒ Ð¿Ð¾Ð²Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»ÑŒÐ½ÐµÐµ Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾Ð³Ð´Ð° Ð¿Ð¾Ð½ÑÐ», Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ Ñ ÑÐ°Ð¼!\n",
        "Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð° ÐºÐ¾Ð½Ñ†Ð° ÑˆÐºÐ¾Ð»Ñ‹ â€“ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ‚Ð°.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð›Ð°Ð´Ð½Ð¾, Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ñ‚Ð°Ðº.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð”Ð° ÑƒÐ¶, _Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð² ÑÑ‚Ñ€ÐµÑÑÐ¾Ð²Ð¾Ð¹ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸_ ÑÐ»Ð¾Ð½Ð° Ð¸ Ð½Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ‚Ð¸Ð»!\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð Ð²Ð¾Ñ‚ Ð²Ð¾Ð¶Ð°Ñ‚Ð°Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ð»Ð° Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ Ð²Ñ‡ÐµÑ€Ð° Ð½Ð¾Ñ‡ÑŒÑŽ Ð¼ÐµÐ½Ñ Ð¾Ñ‚Ñ‡Ð¸Ñ‚Ð°Ð»Ð° Ð·Ð° Ð½ÐµÐ¿Ð¾Ð´Ð¾Ð±Ð°ÑŽÑ‰ÐµÐµ Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ðº Ð½ÐµÐ¹â€¦\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ðš Ñ‡Ñ‘Ñ€Ñ‚Ñƒ!\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð’Ñ€ÑÐ´ Ð»Ð¸ Ð¼Ð¾Ð¹ Ð²Ð½ÐµÑˆÐ½Ð¸Ð¹ Ð²Ð¸Ð´ Ð²Ð»Ð¸ÑÐµÑ‚ Ð½Ð° Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ ÐµÑ‰Ñ‘.\n",
        "Ð•ÑÐ»Ð¸ Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ð°ÑÐ°Ð¼, Ð·Ð°Ð²Ñ‚Ñ€Ð°Ðº ÑƒÐ¶Ðµ Ð´Ð°Ð²Ð½Ð¾ Ð¿Ð¾Ð·Ð°Ð´Ð¸.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐÑƒ Ð»Ð°Ð´Ð½Ð¾, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÑŽ Ð²ÑÑ‘ Ð¶Ðµ Ð² ÑÑ‚Ð¾Ð»Ð¾Ð²Ð¾Ð¹ Ñ‡Ñ‚Ð¾-Ð½Ð¸Ð±ÑƒÐ´ÑŒ Ð½Ð°Ð¹Ñ‚Ð¸.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð’Ñ‡ÐµÑ€Ð° Ð¶Ðµ ÑÐ¾ Ð¡Ð»Ð°Ð²ÐµÐ¹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ.\n",
        "ÐžÑ‚ ÑÑ‚Ð¸Ñ… Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ñ Ð½ÐµÐ²Ð¾Ð»ÑŒÐ½Ð¾ ÑƒÐ»Ñ‹Ð±Ð½ÑƒÐ»ÑÑ.\n",
        "ÐÐ° ÑƒÐ»Ð¸Ñ†Ðµ ÑÑ€ÐºÐ¾ ÑÐ²ÐµÑ‚Ð¸Ð»Ð¾ ÑÐ¾Ð»Ð½Ñ†Ðµ, Ð´ÑƒÐ» Ð»Ñ‘Ð³ÐºÐ¸Ð¹ Ð²ÐµÑ‚ÐµÑ€Ð¾Ðº.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐŸÑ€ÐµÐºÑ€Ð°ÑÐ½Ñ‹Ð¹ Ð»ÐµÑ‚Ð½Ð¸Ð¹ Ð´ÐµÐ½ÑŒ.\n",
        "Ð¯ ÑƒÐ¶Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð»ÐµÑ‚ Ð½Ðµ Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾Ð²Ð°Ð» ÑÐµÐ±Ñ Ð¿Ð¾ ÑƒÑ‚Ñ€Ð°Ð¼ Ñ‚Ð°Ðº Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾.\n",
        "Ð’ÑÐµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð½Ð° ÑÐµÐºÑƒÐ½Ð´Ñƒ ÑƒÐ»ÐµÑ‚ÐµÐ»Ð¸ ÐºÑƒÐ´Ð°-Ñ‚Ð¾ Ð´Ð°Ð»ÐµÐºÐ¾, Ñ€Ð°ÑÑ‚Ð²Ð¾Ñ€Ð¸Ð»Ð¸ÑÑŒ Ð² Ñ€ÐµÐ´ÐºÐ¸Ñ…, Ñ†Ð²ÐµÑ‚Ð° Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐ½ÐµÐ³Ð° Ð¾Ð±Ð»Ð°ÐºÐ°Ñ….\n",
        "Ð’Ð´Ñ€ÑƒÐ³ Ð¿ÐµÑ€ÐµÐ´Ð¾ Ð¼Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð²Ð½Ð¾ Ð¸Ð· Ð½Ð¸Ð¾Ñ‚ÐºÑƒÐ´Ð° Ð¿Ð¾ÑÐ²Ð¸Ð»Ð°ÑÑŒ ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð”Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾, Ð¡ÐµÐ¼Ñ‘Ð½!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð”Ð¾Ð±Ñ€Ð¾Ðµ!\n",
        "Ð¯ ÑƒÐ»Ñ‹Ð±Ð½ÑƒÐ»ÑÑ, Ð¿Ñ‹Ñ‚Ð°ÑÑÑŒ Ð²ÑÐµÐ¼ ÑÐ²Ð¾Ð¸Ð¼ Ð²Ð¸Ð´Ð¾Ð¼ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð½ÐµÑÐ¼Ð¾Ñ‚Ñ€Ñ Ð½Ð¸ Ð½Ð° Ñ‡Ñ‚Ð¾ ÑƒÑ‚Ñ€Ð¾ Ð¼Ð¾Ñ‘ Ð±Ñ‹Ð»Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð¾Ð±Ñ€Ñ‹Ð¼.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð¢Ñ‹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‡ÐµÑ€Ð° Ð¿Ñ€Ð¸ÐµÑ…Ð°Ð», Ñ‚Ð°Ðº Ñ‡Ñ‚Ð¾ Ð±ÑƒÐ´Ð¸Ñ‚ÑŒ Ñ Ñ‚ÐµÐ±Ñ Ð½Ðµ ÑÑ‚Ð°Ð»Ð°, Ð½Ð¾ Ð·Ð°Ð²Ñ‚Ñ€Ð°Ðº-Ñ‚Ð¾â€¦\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð¥Ð¾Ñ‚Ñ Ð»Ð°Ð´Ð½Ð¾! Ð’Ð¾Ñ‚, Ð´ÐµÑ€Ð¶Ð¸!\n",
        "ÐžÐ½Ð° Ð¿Ñ€Ð¾Ñ‚ÑÐ½ÑƒÐ»Ð° Ð¼Ð½Ðµ Ð±ÑƒÐ¼Ð°Ð¶Ð½Ñ‹Ð¹ ÑÐ²Ñ‘Ñ€Ñ‚Ð¾Ðº.\n",
        "Ð¡ÑƒÐ´Ñ Ð¿Ð¾ Ð¼Ð°ÑÐ»ÑÐ½Ñ‹Ð¼ Ð¿ÑÑ‚Ð½Ð°Ð¼, Ð²Ð½ÑƒÑ‚Ñ€Ð¸, ÑÐºÐ¾Ñ€ÐµÐµ Ð²ÑÐµÐ³Ð¾, Ð±ÑƒÑ‚ÐµÑ€Ð±Ñ€Ð¾Ð´Ñ‹.\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: ÐžÐ¹, ÑÐ¿Ð°ÑÐ¸Ð±Ð¾!\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ð°Ñ€Ñˆ ÑƒÐ¼Ñ‹Ð²Ð°Ñ‚ÑŒÑÑ!\n",
        "Ð¯ ÑƒÐ¶Ðµ ÑÐ¾Ð±Ð¸Ñ€Ð°Ð»ÑÑ ÑƒÑ…Ð¾Ð´Ð¸Ñ‚ÑŒ.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð¡ÐµÐ¹Ñ‡Ð°Ñ, Ð¿Ð¾Ð´Ð¾Ð¶Ð´Ð¸.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð° Ð·Ð°Ð±ÐµÐ¶Ð°Ð»Ð° Ð² Ð´Ð¾Ð¼Ð¸Ðº Ð¸, Ð²ÐµÑ€Ð½ÑƒÐ²ÑˆÐ¸ÑÑŒ, ÑÑƒÐ½ÑƒÐ»Ð° Ð¼Ð½Ðµ Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ð¿Ð°ÐºÐµÑ‚Ð¸Ðº.\n",
        "Ð’Ð½ÑƒÑ‚Ñ€Ð¸ Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ÑÑŒ Ð·ÑƒÐ±Ð½Ð°Ñ Ñ‰Ñ‘Ñ‚ÐºÐ°, Ð¼Ñ‹Ð»Ð¾, Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¾Ðµ Ð¿Ð¾Ð»Ð¾Ñ‚ÐµÐ½Ñ†Ðµ Ð¸ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ ÐµÑ‰Ñ‘ â€“ Ñ Ð¾ÑÐ¾Ð±Ð¾ Ð½Ðµ Ð²ÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°Ð»ÑÑ.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: ÐŸÐ¸Ð¾Ð½ÐµÑ€ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð²ÑÐµÐ³Ð´Ð° Ñ‡Ð¸ÑÑ‚ Ð¸ Ð¾Ð¿Ñ€ÑÑ‚ÐµÐ½!\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð”Ð°Ð¹ Ñ Ñ‚ÐµÐ±Ðµ Ð³Ð°Ð»ÑÑ‚ÑƒÐº Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð·Ð°Ð²ÑÐ¶Ñƒ Ð½Ð° Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ€Ð°Ð·, Ð° Ñ‚Ð¾ Ð¾Ð½ Ð±Ð¾Ð»Ñ‚Ð°ÐµÑ‚ÑÑ. ÐŸÐ¾Ñ‚Ð¾Ð¼ Ð½Ð°ÑƒÑ‡Ð¸ÑˆÑŒÑÑ, ÑÐ°Ð¼ Ð±ÑƒÐ´ÐµÑˆÑŒ!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð Ð¼Ð¾Ð¶ÐµÑ‚, Ð½Ðµ Ð½Ð°Ð´Ð¾? Ð¯ ÑÐµÐ¹Ñ‡Ð°Ñ ÑƒÐ¼Ñ‹Ð²Ð°Ñ‚ÑŒÑÑ Ð¸Ð´Ñƒ.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐÑƒ Ð´Ð°, Ð²Ð´Ñ€ÑƒÐ³ Ð·Ð°Ñ†ÐµÐ¿Ð»ÑŽÑÑŒ Ð·Ð° ÐºÑ€Ð°Ð½ Ð¸ ÑƒÐ´Ð°Ð²Ð»ÑŽÑÑŒ...\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð›Ð°Ð´Ð½Ð¾, Ñ‚Ð¾Ð³Ð´Ð° Ð¿Ð¾Ñ‚Ð¾Ð¼. Ð˜ Ð½Ðµ Ð·Ð°Ð±ÑƒÐ´ÑŒ Ð¿Ñ€Ð¾ Ð»Ð¸Ð½ÐµÐ¹ÐºÑƒ.\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐšÐ°Ñ€Ð°Ð½Ð´Ð°ÑˆÐ¸, Ñ€ÑƒÑ‡ÐºÐ¸, Ð»Ð¸Ð½ÐµÐ¹ÐºÐ¸â€¦ Ð¢Ð°ÐºÐ¸Ðµ Ð²ÐµÑ‰Ð¸ Ð½Ðµ Ð·Ð°Ð±Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: ÐšÐ°ÐºÑƒÑŽ Ð»Ð¸Ð½ÐµÐ¹ÐºÑƒ?\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð’ ÑÐ¼Ñ‹ÑÐ»Ðµ â€“ ÐºÐ°ÐºÑƒÑŽ Ð»Ð¸Ð½ÐµÐ¹ÐºÑƒ?!\n",
        "ÐžÐ½Ð° Ð½Ð°Ñ…Ð¼ÑƒÑ€Ð¸Ð»Ð°ÑÑŒ.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð¶Ðµ Ð¿Ð¾Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¸Ðº!\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð¡Ñ‚Ñ€Ð°Ð½Ð½Ð¾, Ð° Ð¿Ð¾ Ð¼Ð¾Ð¸Ð¼ Ð¿Ð¾Ð´ÑÑ‡Ñ‘Ñ‚Ð°Ð¼ â€“ Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµâ€¦\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð’Ð¿Ñ€Ð¾Ñ‡ÐµÐ¼, ÑÐ¼ÐµÐ½Ð° Ð´Ð½Ñ Ð½ÐµÐ´ÐµÐ»Ð¸ â€“ ÑÑ‚Ð¾ ÐµÑ‰Ñ‘ Ð½Ðµ ÑÐ°Ð¼Ð¾Ðµ ÑÑ‚Ñ€Ð°ÑˆÐ½Ð¾Ðµ.\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: ÐžÐ±Ñ‹Ñ‡Ð½Ð¾ Ñƒ Ð½Ð°Ñ Ð»Ð¸Ð½ÐµÐ¹ÐºÐ¸ Ñ€Ð°Ð½Ð¾ ÑƒÑ‚Ñ€Ð¾Ð¼, Ð´Ð¾ Ð·Ð°Ð²Ñ‚Ñ€Ð°ÐºÐ°, Ð½Ð¾ ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¿Ð¾Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¸Ðº, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¾Ð½Ð° Ð±ÑƒÐ´ÐµÑ‚ Ð² 12 Ñ‡Ð°ÑÐ¾Ð².\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: ÐÐµ Ð¾Ð¿Ð°Ð·Ð´Ñ‹Ð²Ð°Ð¹!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð¥Ð¾Ñ€Ð¾ÑˆÐ¾. Ð Ð³Ð´Ðµ?\n",
        "ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°: ÐÐ° Ð¿Ð»Ð¾Ñ‰Ð°Ð´Ð¸, Ð³Ð´Ðµ Ð¶Ðµ ÐµÑ‰Ñ‘!\n",
        "Ð¡Ð¿Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð±Ñ‹Ð»Ð¾ Ð±ÐµÑÑÐ¼Ñ‹ÑÐ»ÐµÐ½Ð½Ð¾.\n",
        "Ð¯ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑÑ Ð² ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ Â«Ð¿Ð¾Ð¼Ñ‹Ð²Ð¾Ñ‡Ð½Ð¾Ð¹Â».\n",
        "ÐÐ° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð´ÑƒÑˆ Ð¸ Ñ‚ÑƒÐ°Ð»ÐµÑ‚ Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½Ðµ Ð¿Ñ€Ð¸Ñ…Ð¾Ð´Ð¸Ð»Ð¾ÑÑŒ, Ð½Ð¾ Ð¿Ñ€Ð¸ Ð²Ð¸Ð´Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð²Ñ‹ÐºÐ¸Ð´Ñ‹ÑˆÐ° Ð·Ð°Ð³Ð½Ð¸Ð²Ð°ÑŽÑ‰ÐµÐ³Ð¾ ÑÐ¾Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¼Ð° â€“ Ð¿Ñ€Ð¸Ñ‡ÑƒÐ´Ð»Ð¸Ð²Ð¾Ð¹ Ñ‡ÐµÑ€ÐµÐ¿Ð°ÑˆÐºÐ¸ Ñ Ð¿Ð°Ð½Ñ†Ð¸Ñ€ÐµÐ¼ Ð¸Ð· Ð¶ÐµÑÑ‚Ð¸, Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾Ð¼ Ð½Ð¾Ð³-ÐºÑ€Ð°Ð½Ð¾Ð² Ð¸ ÐºÐ°Ñ„ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð±Ñ€ÑŽÑˆÐºÐ¾Ð¼ â€“ Ð¼Ð½Ðµ ÑÑ‚Ð°Ð»Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð½Ðµ Ð¿Ð¾ ÑÐµÐ±Ðµ.\n",
        "Ð¯ Ð½Ðµ Ð±Ñ‹Ð» Ð±Ñ€ÐµÐ·Ð³Ð»Ð¸Ð²Ñ‹Ð¼ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¼, Ð½Ð¾ Ñ‚ÐµÐ¼ Ð½Ðµ Ð¼ÐµÐ½ÐµÐµ, ÑÑ‚Ð¾Ñ Ñ‚ÑƒÑ‚, Ð¿Ð¾Ð½ÑÐ», Ñ‡Ñ‚Ð¾ Ð²ÑÑ‘ Ð¶Ðµ ÐµÑÑ‚ÑŒ ÐºÐ°ÐºÐ¾Ð¹-Ñ‚Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð¼Ñ„Ð¾Ñ€Ñ‚Ð°, Ð±ÐµÐ· ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð¶Ð¸Ñ‚ÑŒ Ð¼Ð½Ðµ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾.\n",
        "Ð’Ð¾Ñ‚ Ð²ÐµÐ´ÑŒ ÐºÐ°Ðº Ð±Ñ‹Ð²Ð°ÐµÑ‚ â€“ ÐºÐ¾Ð³Ð´Ð° Ñ‚ÐµÑ€ÑÐµÑˆÑŒ Ð²ÐµÑ‰Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²ÑÐµÐ³Ð´Ð° ÐºÐ°Ð·Ð°Ð»Ð¸ÑÑŒ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾ Ð¾Ð±Ñ‹Ð´ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼Ð¸, Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑˆÑŒ, Ñ‡Ñ‚Ð¾ Ð½Ð° ÑÐ°Ð¼Ð¾Ð¼ Ð´ÐµÐ»Ðµ Ð¾Ð½Ð¸ Ð±Ñ‹Ð»Ð¸ Ð½ÐµÐ·Ð°Ð¼ÐµÐ½Ð¸Ð¼Ñ‹.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð, Ð´Ð° Ð¸ Ñ‡Ñ‘Ñ€Ñ‚ Ñ Ð½Ð¸Ð¼! Ð’Ñ‹Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ Ð½Ðµ Ð¸Ð· Ñ‡ÐµÐ³Ð¾.\n",
        "Ð’Ð¾Ð´Ð° Ð¾ÐºÐ°Ð·Ð°Ð»Ð°ÑÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð»ÐµÐ´ÑÐ½Ð¾Ð¹.\n",
        "Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð¼Ñ‹Ñ‚ÑŒ Ñ€ÑƒÐºÐ¸ Ð½Ðµ ÑÐ¾ÑÑ‚Ð°Ð²Ð¸Ð»Ð¾ Ð¾ÑÐ¾Ð±Ð¾Ð³Ð¾ Ñ‚Ñ€ÑƒÐ´Ð°, Ñ‚Ð¾ Ð²Ð¾Ñ‚ ÑƒÐ¼Ñ‹Ñ‚ÑŒÑÑ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð¿Ð¾Ð»Ð¾ÑÐºÐ°Ñ‚ÑŒ Ñ€Ð¾Ñ‚ ÐµÐ¹ â€“ ÑƒÐ¶Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°.\n",
        "Ð’ Ð¿Ð°ÐºÐµÑ‚Ð¸ÐºÐµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð½Ðµ Ð´Ð°Ð»Ð° ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°, Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¾ÑÑŒ Ð·ÑƒÐ±Ð½Ð¾Ð¹ Ð¿Ð°ÑÑ‚Ñ‹.\n",
        "ÐœÐ¾Ð¶Ð½Ð¾, ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾, Ð±Ñ‹Ð»Ð¾ Ð¿Ð¾Ñ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð·ÑƒÐ±Ñ‹ Ð¸ Ñ‚Ð°Ðº, Ð½Ð¾ Ð² Ð¿Ð¾Ð»Ð¾Ñ‚ÐµÐ½Ñ†Ðµ Ð±Ñ‹Ð»Ð° Ð·Ð°Ð²ÐµÑ€Ð½ÑƒÑ‚Ð° ÐºÐ°ÐºÐ°Ñ-Ñ‚Ð¾ ÐºÑ€ÑƒÐ³Ð»ÐµÐ½ÑŒÐºÐ°Ñ ÐºÐ¾Ñ€Ð¾Ð±Ð¾Ñ‡ÐºÐ°.\n",
        "Â«Ð—ÑƒÐ±Ð½Ð¾Ð¹ Ð¿Ð¾Ñ€Ð¾ÑˆÐ¾ÐºÂ».\n",
        "ÐœÑ‹ÑÐ»Ð¸: ÐŸÑ€ÐµÐ»ÐµÑÑ‚Ð½Ð¾! +1 Ð·Ð° Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ñ Ð³Ð´Ðµ-Ñ‚Ð¾ Ð² Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¼.\n",
        "Ð£Ð¼Ñ‹Ð»ÑÑ Ñ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ð±Ñ‹ÑÑ‚Ñ€Ð¾, Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ð¸ Ð¸Ð·-Ð·Ð° Ð»ÐµÐ´ÑÐ½Ð¾Ð¹ Ð²Ð¾Ð´Ñ‹.\n",
        "ÐšÑ‚Ð¾-Ñ‚Ð¾ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ ÑˆÑ‘Ð», Ð´Ð°Ð¶Ðµ Ð±ÐµÐ¶Ð°Ð» Ð² Ð¼Ð¾ÑŽ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ.\n",
        "Ð¯ Ð¾Ð±ÐµÑ€Ð½ÑƒÐ»ÑÑ.\n",
        "ÐŸÐµÑ€ÐµÐ´Ð¾ Ð¼Ð½Ð¾Ð¹ ÑÑ‚Ð¾ÑÐ»Ð° Ð¡Ð»Ð°Ð²Ñ Ð² ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ ÐºÐ¾ÑÑ‚ÑŽÐ¼Ðµ.\n",
        "ÐŸÐ¾Ñ…Ð¾Ð¶Ðµ, ÑÑ‚Ð° Ð´ÐµÐ²Ð¾Ñ‡ÐºÐ° Ð±ÑƒÐ´ÐµÑ‚ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð²Ñ‹Ð³Ð»ÑÐ´ÐµÑ‚ÑŒ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð¾ Ð²Ð¾ Ð²ÑÑ‘Ð¼ â€“ Ð¸ Ð² Ð¿Ð¸Ð¾Ð½ÐµÑ€ÑÐºÐ¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ðµ, Ð¸ Ð² ÐºÑƒÐ¿Ð°Ð»ÑŒÐ½Ð¸ÐºÐµ, Ð¸, Ð½Ð°Ð²ÐµÑ€Ð½Ð¾Ðµ, Ð´Ð°Ð¶Ðµ Ð² ÐºÐ¾ÑÐ¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼ ÑÐºÐ°Ñ„Ð°Ð½Ð´Ñ€Ðµ.\n",
        "Ð¡Ð»Ð°Ð²Ñ: Ð¤Ð¸Ð·ÐºÑƒÐ»ÑŒÑ‚-Ð¿Ñ€Ð¸Ð²ÐµÑ‚!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: ÐžÑ…Ð°Ð¹â€¦ Ð¢Ð¾ ÐµÑÑ‚ÑŒ, Ð±Ð¾Ð±Ñ€â€¦ Ð”Ð¾Ð±Ñ€Ð¾Ðµ ÑƒÑ‚Ñ€Ð¾! Ð’Ð¾Ñ‚â€¦\n",
        "ÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¼Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð½Ðµ ÑÑ€Ð°Ð·Ñƒ.\n",
        "Ð¡Ð»Ð°Ð²Ñ: ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð½Ð° Ð·Ð°Ð²Ñ‚Ñ€Ð°Ðº Ð½Ðµ Ð¿Ñ€Ð¸ÑˆÑ‘Ð»?\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: ÐŸÑ€Ð¾ÑÐ¿Ð°Ð».\n",
        "Ð¯ ÑÐºÐ°Ð·Ð°Ð» ÑÑ‚Ð¾ Ñ‚Ð°Ðº, ÑÐ»Ð¾Ð²Ð½Ð¾ Ð³Ð¾Ñ€Ð´Ð¸Ð»ÑÑ ÑÐ²Ð¾Ð¸Ð¼ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸ÐµÐ¼.\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: ÐÐ¾ Ð¼Ð½Ðµ ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð° Ð±ÑƒÑ‚ÐµÑ€Ð±Ñ€Ð¾Ð´Ñ‹ Ð¿Ñ€Ð¸Ð½ÐµÑÐ»Ð°.\n",
        "Ð¡Ð»Ð°Ð²Ñ: Ð, Ð½Ñƒ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾ Ñ‚Ð¾Ð³Ð´Ð°! ÐÐµ Ð·Ð°Ð±ÑƒÐ´ÑŒ Ð¿Ñ€Ð¾ Ð»Ð¸Ð½ÐµÐ¹ÐºÑƒ!\n",
        "Ð¡ÐµÐ¼Ñ‘Ð½: Ð”Ð°, ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð—Ð°Ð±ÑƒÐ´ÐµÑˆÑŒ Ñ‚ÑƒÑ‚.\n",
        "Ð¡Ð»Ð°Ð²Ñ: Ð›Ð°Ð´Ð½Ð¾, Ñ Ð¿Ð¾Ð±ÐµÐ¶Ð°Ð»Ð°, Ð½Ðµ ÑÐºÑƒÑ‡Ð°Ð¹!\n",
        "ÐžÐ½Ð° Ð¿Ð¾Ð¼Ð°Ñ…Ð°Ð»Ð° Ð¼Ð½Ðµ Ð½Ð° Ð¿Ñ€Ð¾Ñ‰Ð°Ð½Ð¸Ðµ Ð¸ ÑÐºÑ€Ñ‹Ð»Ð°ÑÑŒ Ð·Ð° Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚Ð¾Ð¼ Ñ‚Ñ€Ð¾Ð¿Ð¸Ð½ÐºÐ¸.\n",
        "ÐœÑ‹ÑÐ»Ð¸: Ð¡ÑƒÐ´Ñ Ð¿Ð¾ Ð²ÑÐµÐ¼Ñƒ, Ð»Ð¸Ð½ÐµÐ¹ÐºÐ° Ð½Ð°Ñ‡Ð½Ñ‘Ñ‚ÑÑ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð°Ñ€Ñƒ Ð¼Ð¸Ð½ÑƒÑ‚.\n",
        "Ð¡Ñ‚Ð¾Ð¸Ð»Ð¾ Ð±Ñ‹ÑÑ‚Ñ€ÐµÐ½ÑŒÐºÐ¾ Ð·Ð°Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Â«Ð´Ð¾Ð¼Ð¾Ð¹Â», Ð·Ð°ÐºÐ¸Ð½ÑƒÑ‚ÑŒ Ð¿Ð°ÐºÐµÑ‚Ð¸Ðº Ñ ÑƒÐ¼Ñ‹Ð²Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸, ÑÑŠÐµÑÑ‚ÑŒ Ð±ÑƒÑ‚ÐµÑ€Ð±Ñ€Ð¾Ð´Ñ‹ Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ¶Ðµ Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð¸Ð´Ñ‚Ð¸ Ð½Ð° Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ.\n",
        "Ð¯ Ñ€Ð°ÑÐ¿Ð°Ñ…Ð½ÑƒÐ» Ð´Ð²ÐµÑ€ÑŒ Ð´Ð¾Ð¼Ð¸ÐºÐ° Ð²Ð¾Ð¶Ð°Ñ‚Ð¾Ð¹ Ð¸ Ð²Ð±ÐµÐ¶Ð°Ð» Ð²Ð½ÑƒÑ‚Ñ€ÑŒ Ñ‚Ð°Ðº, ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ð·Ð°Ð¿Ñ€Ñ‹Ð³Ð¸Ð²Ð°Ð» Ð² Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð²Ð°Ð³Ð¾Ð½ ÑƒÑ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ Ð¿Ð¾ÐµÐ·Ð´Ð°.\n",
        "ÐÐ¾, ÐºÐ°Ð¶ÐµÑ‚ÑÑ, ÑÑ‚Ð¾ Ð±Ñ‹Ð»Ð¾ Ð½Ðµ Ð»ÑƒÑ‡ÑˆÐ¸Ð¼ Ñ€ÐµÑˆÐµÐ½Ð¸ÐµÐ¼ â€“ Ð¿Ð¾ÑÑ€ÐµÐ´Ð¸ ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ñ‹ ÑÑ‚Ð¾ÑÐ»Ð° ÐžÐ»ÑŒÐ³Ð° Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸ÐµÐ²Ð½Ð°â€¦\n",
        "Ð˜ Ð¿ÐµÑ€ÐµÐ¾Ð´ÐµÐ²Ð°Ð»Ð°ÑÑŒ!\n",
        "\"\"\".replace(\"\\n\",\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(prompt)"
      ],
      "metadata": {
        "id": "3L-3e9X6GkHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26804c73-3297-4aa7-8f40-59c248ac6908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7301"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed=123\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "gen(\"Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ\",20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "U--RZUtaivKV",
        "outputId": "952c890a-2fa5-4a2b-8843-3308503a3426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ. Ð¿Ð¾Ð¼Ð¾Ð³Ð¸Ñ‚Ðµ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ñƒ. Ð£ Ð¼ÐµÐ½Ñ ÐµÑÑ‚ÑŒ 2 ÑÐµÑÑ‚Ñ€Ñ‹, Ð¾Ð´Ð½Ð° Ñ€Ð¾Ð´Ð½Ð°Ñ, Ð° Ð´Ñ€ÑƒÐ³Ð°Ñ Ð´Ð²Ð¾ÑŽÑ€Ð¾Ð´Ð½Ð°Ñ.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP77MF8LxKdc"
      },
      "outputs": [],
      "source": [
        "text = prompt\n",
        "while 1:\n",
        "  text=next(text,30)\n",
        "  print(text)\n",
        "  text+=input(\"Ð’Ñ‹:\").replace(\"@\",\"Ð¡ÐµÐ¼Ñ‘Ð½:\").replace(\";;;\",\"\\n\")\n",
        "  text=text[-2000:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture muted\n",
        "!pip install auto-gptq gradio\n",
        "!git clone -b peft_integration https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n",
        "!pip install .[triton]\n",
        "%cd ..\n",
        "#!git clone https://github.com/timdettmers/bitsandbytes.git\n",
        "!pip install bitsandbytes\n",
        "!git clone https://github.com/qwopqwop200/gptqlora\n",
        "%cd gptqlora\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -r requirements.txt\n",
        "!pip install protobuf==3.20.*\n",
        "!pip install huggingface_hub\n",
        "!pip uninstall peft\n",
        "!pip install peft==0.4.0"
      ],
      "metadata": {
        "id": "92-Dv0dSpKEd"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}